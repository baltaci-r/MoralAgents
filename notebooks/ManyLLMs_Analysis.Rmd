---
title: "ManyLLMs: Analysis"
author: "Authors: XXX"
date: "2025-05-21"
output:
  pdf_document:
    toc: true
    number_sections: true
    latex_engine: xelatex
---

<!-- Custom style: works only for HTML output -->
<style type="text/css">
p.author-note {
  font-size: 10pt;
  font-style: italic;
  color: #444;
  margin-top: -1em;
  margin-bottom: 2em;
}
</style>



# Overview


<div class="author-note">
This script accompanies the analyses presented in the manuscript.
Two versions are provided:
</div>

- The `.Rmd` file can be opened and edited in **RStudio**.  
- A rendered **PDF** is available for reference, generated by knitting this file.

---

We provide full R code to ensure the analyses are transparent and reproducible.  
Where necessary, code is explained or unpacked in-text. This document includes:

1. Visualizations of key variable distributions (used in main text and supplementary materials)  
2. Justifications for major analysis decisions

_Last updated: **21 May 2025**_


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  echo = TRUE
)

# Data manipulation
library(dplyr)
library(tidyr)
library(forcats)
library(stringr)
library(broom)
library(tibble)

# Statistical modeling
library(ordinal)
library(emmeans)

# Plotting
library(ggplot2)
library(scales)
library(viridis)
library(patchwork)
library(ggstatsplot)
library(fmsb)

# Tables & output
library(knitr)
library(kableExtra)

# Fonts
library(showtext)

```


First, we recoded factor levels and renamed variables in the dataset to improve readability and streamline the subsequent analysis..  

```{r datasetloading, message=FALSE, warning=FALSE, echo=FALSE}
library(dplyr)
combined_dataset <- readRDS("combined_dataset.rds")
# Make sure 'combined_dataset' exists
stopifnot(exists("combined_dataset"))
# Add a new variable 'Group' based on 'Group'
combined_dataset <- combined_dataset %>%
  mutate(
    Group = ifelse(step == -1, "Solo", "Group"))


#  Prepare and recode types
combined_dataset <- combined_dataset %>%
  mutate(
    type = fct_recode(type,
      "Action–Incongruent"   = "acinc",
      "Omission–Incongruent" = "ininc",
      "Omission–Congruent"   = "incon",
      "Action–Congruent"     = "accon"
    )
  )

combined_dataset <- combined_dataset %>%
  mutate(
    measurement = fct_recode(
      dataset,
      "Factual"             = "korner",
      "Action/Omission"     = "keshmirian",
      "Personal/Impersonal" = "greene",
      "Cons/Norm/Act" = "cni",
      "Harm/Impartial" = "oxford"
    )
  )

combined_dataset <- combined_dataset %>%
  mutate(
    groupsize = fct_recode(
      ob,
      "Solo"             = "n",
      "Pairs"     = "nn",
      "Triads" = "nnn"
  ))
```



## Dataset explaination

Here we summarize the important *Variables* in our dataset.

Our dataset captures each LLM’s response to a moral dilemma in the `opinion` column— a 1–7 *utilitarian score* where higher scores indicate a stronger willingness to endorse the “greater good” at the expense of a moral rule.

The variable `model` identifies the type of *LLMs* which produced this response (e.g.,GPT4.1,Llama3, etc) The field `item` (or `example_index`) numbers the *scenario*, and `rep` indicates the *repetition* for each scenario (1, 2, 3…). 

The field `Group_Size` codes group size (`1` = solo, `2` = pair, `3` = triad). 

The variable `type` classifies the moral dilemma (e.g., “Personal”, “Killing–Util”) within the field `dataset` which names the source questionnaire (e.g., `greene`, `oxfor`, `korner_cni`) which are our measures. This is similar to the variable `measurement`. 

Finally, the variable `Group` denotes the condition: `Solo` for the *single agents* (baseline of the model) and `Group` for the final group‐reflection *consensus*.  The variable `step` is the same, but numerical: Solo (`step -1`) vs Group(`step -1`). Note that `Group` indicates both *pairs* and *triads*. 
**This is our main experimental manipulation.**

We base our analysis on these sets measures to capture  **utilitarian boost** in groups of multi-agent LLM settings. Therefore, in our analysis, we always compare `Group` vs. `Solo` condition. You can see a summary of the variables in Table 1.

```{r glipmse, results='hide'}
glimpse(combined_dataset)
```


```{r variable-definitions, echo=FALSE, message=FALSE, results='asis'}
# Define variable descriptions clearly, without markdown in the cells
var_defs <- data.frame(
  Variable   = c(
    "model",
    "item",
    "rep",
    "group",
    "ob/group_size",
    "type",
    "dataset",
    "opinion"
  ),
  Description = c(
    "LLM that generated the response (e.g., gpt-4.1, llama3.3)",
    "ID of the dilemma scenario - a.k.a expample_index",
    "Repetition index for each scenario (e.g.,1, 2, 3)",
    "Solo (baseline) and  Group (consensus)",
    "Group size: n = solo, nn = pair, nnn = triad",
    "Moral category of the scenario (e.g., Personal, Impersonal, Killing–Util)",
    "Measurement (e.g., greene, oxford, korner, cni)",
    "Utilitarian Score: LLMs’ ratings on a 1–7 Likert scale (higher = more utilitarian)"
  ),
  stringsAsFactors = FALSE
)

# Render a clean two-column table
kable(
  var_defs,
  caption  = "Table: Variable Definitions",
  booktabs = TRUE,
  align    = c("l","l"),
  escape   = FALSE
)

```


## Moral Chage Across LLMs

Together our varriables let us track, for each model × item × phase × repetiion, how the LLM’s moral judgment shifts from Solo to Group.


### Pair vs Solo across all the different measurements

Now We plot Group vs Solo across all the different measurements using an Radar plot.

```{r moral-change-radar2, echo=FALSE, message=FALSE, warning=FALSE}
diff_df <- combined_dataset %>%
  filter() %>%
  group_by(model_short, measurement, ob) %>%
  summarise(mean_op = mean(opinion, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = ob, values_from = mean_op) %>%
  mutate(diff = nn - n) %>%
  select(model_short, measurement, diff)

#  Font setup
font_add_google("Merriweather", "merriweather")
showtext_auto()

# 5) Pivot to wide for radar
radar_df <- diff_df %>%
  pivot_wider(names_from = measurement, values_from = diff, values_fill = 0)

# 6) Add max/min rows (10% padding)
max_vals <- apply(radar_df[-1], 2, max, na.rm = TRUE) * 1.1
min_vals <- apply(radar_df[-1], 2, min, na.rm = TRUE) * 1.1
radar_plot_df <- rbind(max_vals, min_vals, radar_df[-1])
rownames(radar_plot_df) <- c("Max", "Min", radar_df$model_short)

# 7) Define a color‐blind palette
cb_pal      <- c("#0072B2", "#D55E00", "#009E73", "#CC79A7", "#F0E442", "#56B4E9")
n_mod       <- nrow(radar_df)
cols_border <- rep(cb_pal, length.out = n_mod)
cols_fill   <- alpha(cols_border, 0.3)
par(cex.main = 0.8)

# 8) Plot radar chart
fmsb::radarchart(
  radar_plot_df,
  axistype    = 1,
  pcol        = cols_border,
  pfcol       = alpha(cols_border, 0.4),   # lighter fills
  plwd        = 2,
  plty        = 1,
  cglcol      = "grey80",
  cglty       = 1,
  cglwd       = 2,                          # thicker web lines
  axislabcol  = "black",                    # darker labels
  caxislabels = round(seq(min(min_vals), max(max_vals), length.out = 5), 2),
  vlcex       = 0.9,
  seg         = 4,
  title       = NULL
)

title("Moral Change Profile Across LLMs\n(Pair → Solo)")


# 9) Add legend
legend(
  x       = "topright",
  legend  = radar_df$model_short,
  bty     = "n",
  pch     = 20,
  col     = cols_border,
  text.col= "black",
  cex     = 1,
  pt.cex  = 3
)
```
### Pair vs Solo across all the different types


This shows *moral change* profiles across different models for different types The difference is the `Pairs` - `Solo`. 

```{r, echo=FALSE}
diff_df <- combined_dataset %>%
  filter(ob %in% c("n","nn")) %>%
  group_by(model_short, type, ob) %>%
  summarise(mean_op = mean(opinion, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = ob, values_from = mean_op) %>%
  mutate(diff = nn - n) %>%
  select(model_short, type, diff)
#  Font setup
font_add_google("Merriweather", "merriweather")
showtext_auto()

# 5) Pivot to wide for radar
radar_df <- diff_df %>%
  pivot_wider(names_from = type, values_from = diff, values_fill = 0)

# 6) Add max/min rows (10% padding)
max_vals <- apply(radar_df[-1], 2, max, na.rm = TRUE) * 1.1
min_vals <- apply(radar_df[-1], 2, min, na.rm = TRUE) * 1.1
radar_plot_df <- rbind(max_vals, min_vals, radar_df[-1])
rownames(radar_plot_df) <- c("Max", "Min", radar_df$model_short)

# 7) Define a color‐blind palette
cb_pal      <-  c("#8B0000", "#D2691E", "#FFD700", "#9ACD32", "#556B2F", "#8B4513")
n_mod       <- nrow(radar_df)
cols_border <- rep(cb_pal, length.out = n_mod)
cols_fill   <- alpha(cols_border, 0.3)
par(cex.main = 0.8)

# 8) Plot radar chart
fmsb::radarchart(
  radar_plot_df,
  axistype    = 1,
  pcol        = cols_border,
  pfcol       = alpha(cols_border, 0.2),   # lighter fills
  plwd        = 2,
  plty        = 1,
  cglcol      = "grey80",
  cglty       = 1,
  cglwd       = 2,                          # thicker web lines
  axislabcol  = "black",                    # darker labels
  caxislabels = round(seq(min(min_vals), max(max_vals), length.out = 5), 2),
  vlcex       = 0.9,
  seg         = 4,
  title       = "Moral Change Profile Across LLMS\n(Pair -> Solo)"
)


# 9) Add legend
legend(
  x       = "topright",
  legend  = radar_df$model_short,
  bty     = "n",
  pch     = 20,
  col     = cols_border,
  text.col= "black",
  cex     = 1,
  pt.cex  = 3
)
```
### Group vs Solo across all the different types


This shows *moral change* profiles across different models. The difference is the `Groups (pairs and triads)` - `Solo`.

```{r moral-change-radar221, echo=FALSE, message=FALSE, warning=FALSE}
diff_df2 <- combined_dataset %>%
  filter() %>%
  group_by(model_short, type, Group) %>%
  summarise(mean_op = mean(opinion, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = Group, values_from = mean_op) %>%
  mutate(diff = Group - Solo) %>%
  select(model_short, type, diff)

# 5) Pivot to wide for radar
radar_df <- diff_df %>%
  pivot_wider(names_from = type, values_from = diff, values_fill = 0)

# 6) Add max/min rows (10% padding)
max_vals <- apply(radar_df[-1], 2, max, na.rm = TRUE) * 1.1
min_vals <- apply(radar_df[-1], 2, min, na.rm = TRUE) * 1.1
radar_plot_df <- rbind(max_vals, min_vals, radar_df[-1])
rownames(radar_plot_df) <- c("Max", "Min", radar_df$model_short)

# 7) Define a color‐blind palette
cb_pal      <- c("#ff6f61", "#6b5b95", "#88b04b", "#f7cac9", "#92a8d1", "#955251")
n_mod       <- nrow(radar_df)
cols_border <- rep(cb_pal, length.out = n_mod)
cols_fill   <- alpha(cols_border, 0.5)
par(cex.main = 0.8)

# 8) Plot radar chart
fmsb::radarchart(
  radar_plot_df,
  axistype    = 1,
  pcol        = cols_border,
  pfcol       = alpha(cols_border, 0.1),   # lighter fills
  plwd        = 2,
  plty        = 1,
  cglcol      = "grey80",
  cglty       = 1,
  cglwd       = 2,                          # thicker web lines
  axislabcol  = "black",                    # darker labels
  caxislabels = round(seq(min(min_vals), max(max_vals), length.out = 5), 2),
  vlcex       = 0.9,
  seg         = 4,
  title       = "Moral Change Profile Across LLMs\n(Group – Solo)"
)


# 9) Add legend
legend(
  x       = "topright",
  legend  = radar_df$model_short,
  bty     = "n",
  pch     = 20,
  col     = cols_border,
  text.col= "black",
  cex     = 1,
  pt.cex  = 3
)
```


We see the *profile* of moral change is different across models and types. Next we See each measurement in more depth. 

# Utilitarain Moral Dilemmas

Our **primary reference** is the classic set of *moral utilitarian dilemmas* (`dataset == "greene").  


```{r split-datasets, message=FALSE, warning=FALSE}
# Recreate the Greene subset for this document
greene_df <- combined_dataset %>%
  filter(dataset == "greene") %>%
  droplevels()
```

Preparing Data for Ordinal Modeling: We begin by extracting only the Greene subset and converting our response variable to an ordered factor:

```{r prepare-data2}
# Prepare dataset
reflection_moral <- greene_df
reflection_moral$opinion <- as.ordered(reflection_moral$opinion)

```


## Models 


The **ordinal** package[^1] provides functions for fitting *cumulative link models* (`clm()`) and *cumulative link mixed models* (`clmm()`) to **ordinal** response data. It supports both fixed-effects formulae and random-effects structures, making it ideal for analyzing Likert-type outcomes with clustered or repeated measures.

[^1]: Christensen, R. H. B. (2019). *ordinal: Regression Models for Ordinal Data*. R package version 2019.12-10.  
&nbsp;&nbsp;&nbsp;https://CRAN.R-project.org/package=ordinal  


Next, we fit three candidate cumulative link mixed models to determine the optimal random‐effects structure. Each model includes the fixed effect of `Group` (factor type of step) but varies in its random terms:

*Note:* The variable `rep` denotes **repetition**—each scenario is repeated multiple times to ensure consistency and robustness of the judgments.


```{r model 1,cache = TRUE}
model1 <- clmm(
  opinion ~ Group + (1 | item),
  data  = reflection_moral,
  Hess  = TRUE
)
```


1. **Model 1:** random intercept for each `item`  
   This baseline model uses `(1 | item)` to allow every dilemma scenario to have its own starting point on the 1–7 scale. It assumes that the effect of `Group` (single vs. group) is **constant** across all items, but accounts for inherent differences in how “utilitarian” each scenario tends to be.




```{r Model 2,cache = TRUE}
model2 <- clmm(
  opinion ~ Group + (rep | item),
  data  = reflection_moral,
  Hess  = TRUE
)
```

2. **Model 2:** random slope of repetitions `rep` within each `item`  
In addition to item intercepts, this model lets each scenario exhibit its own pattern across repeated presentations. Some items may elicit more consistent ratings across repeats, while others show greater variability, capturing item-specific response stability.


```{r Model 3,cache = TRUE}
model3 <- clmm(
  opinion ~ Group + (1 | item)+ (1 | rep),
  data  = reflection_moral,
  Hess  = TRUE
)
```

3. **Model 3:** crossed random intercepts for `item` and `rep`  
This model builds on the previous ones by adding a separate random intercept for each repetitions (`rep`) in addition to the scenario‐specific intercepts for each `item`. This accounts for two sources of baseline variation:  
     - **Item-level:** some dilemmas are judged more utilitarian or deontological on average.  
     - **Repetition-level:** certain runs (e.g., the first, second, third presentation) may systematically differ.
     
     
## Models Comparison and Selection

**Model Comparison:** with *Likelihood‐Ratio* Test: We compare the three fitted `clmm` models using `anova()` to perform a *likelihood‐ratio* test. This assesses whether each increase in complexity of the random‐effects structure leads to a statistically significant improvement in model fit.

```{r Model Comparison,cache = TRUE}
anova_res <- anova(model1, model2, model3)
```



```{r model-comparison, echo=FALSE, message=FALSE, warning=FALSE, results='asis',cache = TRUE}
# Compare models

# Convert ANOVA results to a data.frame
anova_df <- as.data.frame(anova_res)

# Build a clean summary table
anova_tbl <- data.frame(
  Model        = c("Model 1", "Model 3", "Model 2"),
  AIC          = round(anova_df$AIC, 2),
  `Chi-square` = round(anova_df$LR.stat, 2),
  df           = anova_df$df,
  `p-value`    = signif(anova_df$`Pr(>Chisq)`, 4),
  check.names  = FALSE,
  stringsAsFactors = FALSE
)

# Render as a PDF-friendly table
kable(
  anova_tbl,
  caption  = "Table: Likelihood-Ratio Test for Random-Effects Structures",
  booktabs = TRUE,
  align    = c("l", "r", "r", "r", "r")
)

```

### Model Selection

Model 2 is the preferred model. The likelihood‐ratio test comparing Model 2 to Model 1 yields **p = `r signif(anova_res[2, 'Pr(>Chisq)'], 3)`**, indicating a significant improvement in fit. Additionally, Model 2 reduces the AIC by `r round(AIC(model1) - AIC(model2), 2)` and the BIC by `r round(BIC(model1) - BIC(model2), 2)` relative to Model 1. These metrics together demonstrate that allowing item‐specific repetition effects (Model 2) provides a more parsimonious and better‐fitting model.



## Utilitarian Boost

Model 2 was selected as the optimal random‐effects structure. Below we summarize its **fixed‐effect estimates** and then report **pairwise comparisons** of reflection phases on the probability scale.

We now report the fixed‐effect estimates and pairwise comparisons for Model 2 in Table 2.

```{r, include = FALSE, cache = TRUE, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
# 3. Summarize the fitted model
summary(model2)

# 4. Obtain pairwise estimated marginal means (on the probability scale)
emms <- emmeans(
  model2,
  pairwise ~ Group ,
  type = "response"
)
```


```{r model2-summary-emms2, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
library(broom)
library(dplyr)
library(knitr)
library(emmeans)

# 1. Fixed‐effect estimates for Model 2
fixed_eff <- broom::tidy(model2, effects = "fixed") %>%
  select(term, estimate, std.error, statistic, p.value) %>%
  rename(
    Term         = term,
    Estimate     = estimate,
    `Std. Error` = std.error,
    `z value`    = statistic,
    `p value`    = p.value
  )

kable(
  fixed_eff,
  caption  = "Table: Fixed‐Effect Estimates for Model 2",
  booktabs = TRUE,
  align    = c("l","r","r","r","r"),
  digits   = 5
)
```

```{r summ, include = TRUE, cache = TRUE, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
# 5. View contrast summaries
summary(emms$contrasts)
```

## Personal vs. Impersonal

Our **primary reference** is the classic set of *moral sacrificial dilemmas* (`dataset == "greene"`), which we analyze across two key dimensions:  
- `type == "Personal"` vs. `type == "Impersonal"` 

Using our winning random‐effects structure, we now test whether the **effect of Group (`Group`)** differs across **scenario types** (`type`). We fit a cumulative link mixed model with an interaction between `Group` and `type`, retaining a random intercept for each dilemma item (`item`). 

```{r,cache = TRUE,message=FALSE, reslut='hide', echo=TRUE}
#Fit cumulative link mixed model (CLMM)
model_clmm <- clmm(
  opinion ~ Group * type +
    (1| item),
  data = reflection_moral,
  Hess = TRUE
)

# Summarize the fitted model
summary(model_clmm)
```

We then extract **pairwise estimated marginal means** on the probability scale and present the **contrast** results in a PDF‐friendly table.

```{r,cache = TRUE,message=FALSE}
# 4. Obtain pairwise estimated marginal means (on the probability scale)
emms <- emmeans(
  model_clmm,
  pairwise ~ Group * type ,
  type = "response"
)
# 5. View contrast summaries
summary(emms$contrasts)
```
P value adjustment: tukey method for comparing a family of 4 estimates

Wihtout rep: 
(Group-1 Impersonal) - Group7 Impersonal     0.111 0.0594 Inf   1.874  0.2393
 (Group-1 Impersonal) - (Group-1 Personal)    2.616 0.9380 Inf   2.789  0.0271
 (Group-1 Impersonal) - Group7 Personal       2.021 0.9390 Inf   2.154  0.1364
 Group7 Impersonal - (Group-1 Personal)       2.504 0.9390 Inf   2.667  0.0383
 Group7 Impersonal - Group7 Personal          1.910 0.9400 Inf   2.033  0.1760
 (Group-1 Personal) - Group7 Personal        -0.594 0.0507 Inf -11.728  <.0001

```{r personal-imp-contrasts, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
library(dplyr)
library(stringr)
library(knitr)

# Extract contrasts as a data frame
contrast_df <- as.data.frame(summary(emms$contrasts))

# Keep only comparisons where both levels share the same type
# (i.e. both “Impersonal” or both “Personal”), and drop the df column
filtered_contrasts <- contrast_df %>%
  filter(
    str_detect(contrast, "Impersonal.*Impersonal") |
    str_detect(contrast, "Personal.*Personal")
  ) %>%
  select(contrast, estimate, SE, z.ratio, p.value)

# Render a simple table
kable(
  filtered_contrasts,
  caption = "Contrasts Within Type (Impersonal and Personal)",
  digits  = 5,
  booktabs = TRUE
)

```


We see that **Group** opinions are significantly higher than **Solo** opinions across all LLMs for **Personal** dilemmas.



##  Pairs and Triads: group size effect.

In our previous models, including a random slope for **repetition** (`rep`) caused convergence failures, so we removed `rep` from the random structure and now focus on how **group size** (`ob`) affects moral judgments.

Because only the **Personal** dilemma type showed a significant interaction with reflection phase, we restrict our analysis to **Personal** scenarios. We then fit a cumulative link mixed model with:

- **Fixed effects:** interaction of `Group` (“Solo” vs. “Group”) and `ob` (n, nn, nnn)  
- **Random intercepts:** for each scenario item (`item`)  




```{r,cache = TRUE, results= 'hide' }

# Fit cumulative link mixed model (CLMM) 
model_clmm <- clmm(
  opinion ~ Group * groupsize  +
    (rep| item),
  data = reflection_moral,
  Hess = TRUE
)

# 3. Summarize the fitted model
summary(model_clmm)
```


```{r,cache = TRUE}
# 4. Obtain pairwise estimated marginal means (on the probability scale)
emms <- emmeans(
  model_clmm,
  pairwise ~ Group * groupsize ,
  type = "response"
)
```


```{r,cache = TRUE,echo=TRUE, results='hide'}
# 5. View contrast summaries
summary(emms$contrasts)
```

 contrast                 estimate     SE  df z.ratio p.value
 (n Group-1) - nn Group7    -0.712 0.0815 Inf  -8.732  <.0001
 (n Group-1) - nnn Group7   -0.572 0.0709 Inf  -8.061  <.0001
 nn Group7 - nnn Group7      0.140 0.0904 Inf   1.546  0.2696

P value adjustment: tukey method for comparing a family of 3 estimates 

```{r, echo=FALSE}
library(knitr)

# View contrast results
contrast_summary <- summary(emms$contrasts)

# Print as markdown table
kable(contrast_summary, digits = 3, caption = "Estimated Marginal Means Contrasts (Tukey-adjusted)")

```


We see that the effect is significant for Solo vs both Group Sizes (pairs and triads). No difference between Pairs and Triads.  The plot below show this difference across different items.  


## Utilitarian Boost in LLMs

Here we show the effect across each model for the Personal dilemmas. Note that we use  *fit_mod <- clmm(opinion ~ Group + (1 | item)* since the *rep|item* does not converge due to negative hessian.


```{r personal-contrast-kable1, echo=TRUE, message=FALSE, warning=FALSE, results='asis',cache=TRUE}
# List of models to compare
models_to_compare <- c("Gemma3", "Lamma3.3", "Qwen2.5", "Qwen3", "QWQ")

# Function to fit the model and extract contrasts
get_contrast <- function(m) {
  df_mod <- reflection_moral %>%
    filter(type == "Personal", model_short == m)
  fit_mod <- clmm(opinion ~ Group + (1 | item), data = df_mod, Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ Group, type = "response")
as.data.frame(summary(em$contrasts)) %>%
    transmute(
      Model        = m,
      Contrast     = contrast,
      Estimate     = estimate,
      `Std. Error` = SE,
      `z value`    = z.ratio,
      `p value`    = p.value
    )
}

contrast_tbl <- bind_rows(lapply(models_to_compare, get_contrast))
```



```{r, echo=FALSE, results='hide'}
contrast_tbl
```


```{r personal-contrast-kab, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}

# Run for all models and bind results

kable(contrast_tbl, digits = 5, caption = "Table: Group vs. Solo Contrast for Personal Dilemmas Across LLMs")

```

Since the Hessian for `GPT4.1` was not positive definite (i.e.  `Error in vcov.clm(...): Hessian is not positive definite`), we instead **pool across both group sizes** and include an interaction term. For `GPT4.1`


```{r, cache=TRUE}
reflection_moral_gpt=filter(reflection_moral, model_short=="GPT4.1" & type == "Personal")
fit_mod <- clmm(opinion ~ Group * ob + (1 | item), data = reflection_moral_gpt , Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ Group * ob, type = "response")
```

```{r emmeans-gpt4, echo=FALSE, results='asis', cache=TRUE}
library(knitr)

# Convert the emmeans object to a data.frame
em_means <- as.data.frame(em$contrasts)

# Render a simple kable
kable(
  em_means,
  digits  = 3,
  caption = "Estimated Marginal Means for GPT4.1 (Personal) Solo vs Group"
)
```


## Baseline and Utilitarian Boost

Here we report some extra analyses on the LLM comparisons, focusing on: **Baseline model of utilitarianism**  Examining both the personal dilemmas (`type == "Personal"`) and the overall utilitarian tendency. **Utilitarian shift of the LLMs**  - Contrasts between solo and group responses (`Group` effect) across models.  **Sanity checks**  No difference between agents in `group_size == 2` vs. `group_size == 3`.Sample‐balanced version of the analysis using `filter(rep < 15)` to ensure equal representation across group sizes. 





### Baseline utilitarianism of LLMs


```{r mean-op-boxplot-statr,cache=TRUE,echo=FALSE, message=FALSE, warning=FALSE}
diff_df2 <- greene_df %>%
  filter(ob=="n", rep<10) %>%
  group_by(type,model_short,example_index,rep) %>%
  summarise(mean_op = mean(opinion, na.rm = TRUE), .groups = "drop")

library(ggstatsplot)

ggwithinstats(
  data          = filter(diff_df2, type == "Personal"),
  x                     = model_short,         
  y                     = mean_op,
  plot.type             = "box",           # box plots only
  type                  = "Parametric",     
  pairwise.comparisons  = TRUE,           # no p-values
  messages              = FALSE,
  effsize.type	= "eta",
  bf.message= TRUE,
  results.subtitle= TRUE,
  xlab =  "Group Size",
  pairwise.display = "significant",ylab = "Utilitarian Score",
  caption = "Utilitarain Score across different group sizes",
      # suppress messages)
title = "Utilitarian Baseline Across Items (Single-Agent)",
subtitle = "Size 1 (Solo), Group Size 2 (Pairs) and Group Size 3 (Triads)",
centrality.plotting = TRUE,
  centrality.point.args = list(size = 5, color = "darkred"),
  centrality.label.args = list(size = 3, nudge_x = 0.4, segment.linetype = 4),
  centrality.path = TRUE,
  centrality.path.args = list(linewidth = 2, color = "red", alpha = 0.5),
  point.args = list(size = 4, alpha = 0.3, na.rm = TRUE),
  point.path = TRUE,
  point.path.args = list(alpha = 0.5, linetype = "dashed"),
  boxplot.args = list(width = 0.2, alpha = 0.4, na.rm = TRUE),
  violin.args = list(width = 0.0, alpha = 0.5, na.rm = TRUE),
  ggsignif.args = list(textsize = 2, tip_length = 0.01, na.rm = TRUE),
  ggtheme = ggplot2::theme_minimal(base_size = 12),
  package = "RColorBrewer",
  palette = "Set2",
  ggplot.component = NULL)

```



Here we fit a cumulative link mixed‐effects model (CLMM) to estimate and compare the baseline differences across our “model” factor *different LLMs*, using  the Personal condition at the Solo step (`step == -1`). We include a random intercept for each item to account for item‐level variability.

```{r fit-personal-clmm, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
library(dplyr)
library(ordinal)

# 1) Subset to Personal & Solo (step == -1)
reflection_moral_pb <- reflection_moral %>%
  filter(type == "Personal", step == -1)

# 2) Fit CLMM with random intercepts for item
llm_clmm <- clmm(
  opinion ~ model + (1 | item),
  data = reflection_moral_pb,
  Hess = TRUE
)

summary(llm_clmm)

```

Here we obtain the pairwise estimated marginal means

```{r,cache = TRUE}
# 4. Obtain pairwise estimated marginal means 
emmsmodel <- emmeans(
  llm_clmm,
  pairwise ~ model,
  type = "response"
)

# 5. View contrast summaries
summary(emmsmodel$contrasts)
```


```{r personal-contrast-kab2, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
# Run for all models and bind results

kable(emmsmodel$contrasts, digits = 3, caption = "Table: Group vs. Solo Contrast for Personal Dilemmas Across LLMs")

```


Models are different from each other with respect to baseline. We use `GGStatplot` to show this difference. 

```{r solo-violin-with-means, echo=FALSE, message=FALSE, warning=FALSE}
# 0) Font setup
library(showtext)
font_add_google("Merriweather", "merriweather")
showtext_auto()

# 1) Prep data: only Solo (step == -1) & Personal dilemmas, reorder by mean opinion
library(dplyr)
library(forcats)
df_solo <- greene_df %>%
  filter(type == "Personal", step == -1) %>%
  mutate(
    model_short = fct_reorder(model_short, opinion, .fun = mean)
  )

# Compute mean per model
means_df <- df_solo %>%
  group_by(model_short) %>%
  summarise(mean_opinion = mean(opinion, na.rm = TRUE), .groups = "drop")

# 2) Plot: violin per model, solo only, with mean labels in a styled box
library(ggplot2)
library(viridis)

ggplot(df_solo, aes(x = model_short, y = opinion, fill = model_short)) +
  geom_violin(trim = FALSE, alpha = 0.6) +
  geom_point(
    data = means_df,
    aes(y = mean_opinion),
    color = "white", fill = "red", shape = 21, size = 3
  ) +
  geom_label(
    data = means_df,
    aes(y = mean_opinion, label = sprintf("%.2f", mean_opinion)),
    family   = "merriweather",
    fontface = "bold",
    size     = 3,
    fill     = "lightyellow",
    color    = "black",
    label.size = 0.5,
    label.r    = unit(0.2, "lines"),
    vjust    = -0.7
  ) +
  scale_fill_viridis_d(option = "C", begin = 0.2, end = 0.8, guide = "none") +
  labs(
    title    = "Solo Condition: Opinion Distribution by Model",
    subtitle = "Personal Dilemmas",
    x        = "Model (ordered by mean opinion)",
    y        = "Utilitarian Score (1–7)"
  ) +
  theme_minimal(base_family = "merriweather", base_size = 14) +
  theme(
    plot.title       = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle    = element_text(size = 12, hjust = 0.5, margin = margin(b = 10)),
    axis.text.x      = element_text(angle = 45, hjust = 1, size = 10),
    axis.title.y     = element_text(size = 12),
    panel.grid.minor = element_blank(),
    legend.position  = "none",
    plot.margin      = margin(20, 20, 20, 20)
  ) +
  scale_y_continuous(limits = c(1, 7), breaks = 1:7)

```
```{r}
# 0) Font setup
library(showtext)
font_add_google("Merriweather", "merriweather")
showtext_auto()

# 1) Prep data: only Solo (step == -1) & Personal dilemmas, reorder by mean opinion
library(dplyr)
library(forcats)
df_solo <- greene_df %>%
  filter( step == -1) %>%
  mutate(
    model_short = fct_reorder(model_short, opinion, .fun = mean)
  )

# Compute mean per model
means_df <- df_solo %>%
  group_by(model_short) %>%
  summarise(mean_opinion = mean(opinion, na.rm = TRUE), .groups = "drop")

# 2) Plot: violin per model, solo only, with mean labels in a styled box
library(ggplot2)
library(viridis)

ggplot(df_solo, aes(x = model_short, y = opinion, fill = model_short)) +
  geom_violin(trim = FALSE, alpha = 0.6) +
  geom_point(
    data = means_df,
    aes(y = mean_opinion),
    color = "white", fill = "red", shape = 21, size = 3
  ) +
  geom_label(
    data = means_df,
    aes(y = mean_opinion, label = sprintf("%.2f", mean_opinion)),
    family   = "merriweather",
    fontface = "bold",
    size     = 3,
    fill     = "lightyellow",
    color    = "black",
    label.size = 0.5,
    label.r    = unit(0.2, "lines"),
    vjust    = -0.7
  ) +
  scale_fill_viridis_d(option = "C", begin = 0.2, end = 0.8, guide = "none") +
  labs(
    title    = "Solo Condition: Opinion Distribution by Model",
    subtitle = "Sacrificial Dilemmas",
    x        = "Model (ordered by mean opinion)",
    y        = "Utilitarian Score (1–7)"
  ) +
  theme_minimal(base_family = "merriweather", base_size = 14) +
  theme(
    plot.title       = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle    = element_text(size = 12, hjust = 0.5, margin = margin(b = 10)),
    axis.text.x      = element_text(angle = 45, hjust = 1, size = 10),
    axis.title.y     = element_text(size = 12),
    panel.grid.minor = element_blank(),
    legend.position  = "none",
    plot.margin      = margin(20, 20, 20, 20)
  ) +
  scale_y_continuous(limits = c(1, 7), breaks = 1:7)

```

Next we show this difference for Impersonal scenarios: 
```{r}
# 0) Font setup
font_add_google("Merriweather", "merriweather")
showtext_auto()

# 1) Prep data: only Solo (step == -1) & Personal dilemmas, reorder by mean opinion
library(dplyr)
library(forcats)
df_solo <- greene_df %>%
  filter( type!="Personal",step == -1) %>%
  mutate(
    model_short = fct_reorder(model_short, opinion, .fun = mean)
  )

# Compute mean per model
means_df <- df_solo %>%
  group_by(model_short) %>%
  summarise(mean_opinion = mean(opinion, na.rm = TRUE), .groups = "drop")

# 2) Plot: violin per model, solo only, with mean labels in a styled box
library(ggplot2)
library(viridis)

ggplot(df_solo, aes(x = model_short, y = opinion, fill = model_short)) +
  geom_violin(trim = FALSE, alpha = 0.6) +
  geom_point(
    data = means_df,
    aes(y = mean_opinion),
    color = "white", fill = "red", shape = 21, size = 3
  ) +
  geom_label(
    data = means_df,
    aes(y = mean_opinion, label = sprintf("%.2f", mean_opinion)),
    family   = "merriweather",
    fontface = "bold",
    size     = 3,
    fill     = "lightyellow",
    color    = "black",
    label.size = 0.5,
    label.r    = unit(0.2, "lines"),
    vjust    = -0.7
  ) +
  scale_fill_viridis_d(option = "C", begin = 0.2, end = 0.8, guide = "none") +
  labs(
    title    = "Solo Condition: Opinion Distribution by Model",
    subtitle = "Impersonal Dilemmas",
    x        = "Model (ordered by mean opinion)",
    y        = "Utilitarian Score (1–7)"
  ) +
  theme_minimal(base_family = "merriweather", base_size = 14) +
  theme(
    plot.title       = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle    = element_text(size = 12, hjust = 0.5, margin = margin(b = 10)),
    axis.text.x      = element_text(angle = 45, hjust = 1, size = 10),
    axis.title.y     = element_text(size = 12),
    panel.grid.minor = element_blank(),
    legend.position  = "none",
    plot.margin      = margin(20, 20, 20, 20)
  ) +
  scale_y_continuous(limits = c(1, 7), breaks = 1:7)
```

## Sanity Checks

### Utilitarian score in Personal vs Impersonal: 

Personal ratings should be lower than Impersonal scenarios (both LLM and Human prior studies showed that). So we look at the baseline differences for each group. Here we fit a cumulative link mixed‐effects model (CLMM) to estimate and compare the  differences across our “type” factor *Personal vs Impersonal*, using  the both typles at the Solo step (`step == -1`). 


```{r type-clmm, cache=TRUE, message=FALSE, warning=FALSE, result='hide'}
reflection_moral_pb=filter(reflection_moral, step==-1)
# Fit cumulative link mixed model (CLMM) with random intercepts for example_index
type_clmm <- clm(
  opinion ~ type,
  data = reflection_moral_pb,
  Hess = TRUE
)

summary(type_clmm)
```
We obtain pairwise estimated marginal means (on the probability scale)

```{r,cache = TRUE}
# 4. Obtain pairwise estimated marginal means (on the probability scale)
emmstype <- emmeans(
  type_clmm,
  pairwise ~ type,
  type = "response"
)
```


```{r,cache = TRUE,echo=FALSE,include=FALSE}
# 5. View contrast summaries
summary(emmstype$contrasts)
```


```{r personal-contrast-kable2, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
# Run for all models and bind results

kable(emmstype$contrasts, digits = 3, caption = "Table: Group vs. Solo Contrast for Personal Dilemmas Across LLMs")

```


We see that in general there is a significant difference between the type which can be seen the the plot below: 


```{r mean-op-boxplot-stat41,cache=TRUE,echo=FALSE, message=FALSE, warning=FALSE}
diff_df2 <- greene_df %>%
  filter(ob=="n") %>%
  group_by(type,example_index) %>%
  summarise(mean_op = mean(opinion, na.rm = TRUE), .groups = "drop")

library(ggstatsplot)

ggwithinstats(
  data          = filter(diff_df2, ),
  x                     = type,         
  y                     = mean_op,
  plot.type             = "box",           # box plots only
  type                  = "Parametric",     
  pairwise.comparisons  = TRUE,           # no p-values
  messages              = FALSE,
  effsize.type	= "eta",
  bf.message= TRUE,
  results.subtitle= TRUE,
  xlab =  "Group Size",
  pairwise.display = "significant",ylab = "Utilitarian Score",
  caption = "",
title = "Utilitarian Baseline Across Types (Single-Agent)",
subtitle = "",
centrality.plotting = TRUE,
  centrality.point.args = list(size = 5, color = "darkred"),
  centrality.label.args = list(size = 3, nudge_x = 0.4, segment.linetype = 4),
  centrality.path = TRUE,
  centrality.path.args = list(linewidth = 2, color = "red", alpha = 0.5),
  point.args = list(size = 4, alpha = 0.3, na.rm = TRUE),
  point.path = TRUE,
  point.path.args = list(alpha = 0.5, linetype = "dashed"),
  boxplot.args = list(width = 0.2, alpha = 0.4, na.rm = TRUE),
  violin.args = list(width = 0.0, alpha = 0.5, na.rm = TRUE),
  ggsignif.args = list(textsize = 2, tip_length = 0.01, na.rm = TRUE),
  ggtheme = ggplot2::theme_minimal(base_size = 12),
  package = "RColorBrewer",
  palette = "Set1",
  ggplot.component = NULL)

```

### Sanity Check across models 

Now we extend this to models. We see the same pattern across all models. 

```{r type-clmm-llm, cache=TRUE, message=FALSE, warning=FALSE}
reflection_moral_pb=filter(reflection_moral, step==-1)
# Fit cumulative link mixed model (CLMM) with random intercepts for example_index
#    and a random slope of 'step' by example_index:model to mirror (model | example_index)
typellm_clmm <- clm(
  opinion ~ type * model_short,
  data = reflection_moral_pb,
  Hess = TRUE
)
```



```{r,cache = TRUE, echo=FALSE, results='hide'}
# 3. Summarize the fitted model
summary(typellm_clmm)

# 4. Obtain pairwise estimated marginal means (on the probability scale)
emmstype <- emmeans(
  typellm_clmm,
  pairwise ~ model_short * type,
  type = "response"
)
```

```{r personal-vs-impersonal-same-model1, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
library(dplyr)
library(knitr)

# 1) Get contrasts as a data frame
contrast_df <- as.data.frame(summary(emmstype$contrasts))

# 2) Keep only "Model Impersonal - Model Personal" for each model
pi_df <- contrast_df %>%
  filter(grepl("^(\\S+) Impersonal - \\1 Personal$", contrast))
```


```{r personal-vs-impersonal-same-model, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
# 3) Render with kable
kable(
  pi_df,
  caption  = "Impersonal vs. Personal Contrast (same model)",
  digits   = 4,
  booktabs = TRUE,
  col.names = c("Contrast", "Estimate", "Std. Error", "DF", "z value", "p value")
)
```


The plot shows the Sanity Checks across models. 

```{r impersonal-vs-personal-violin, echo=FALSE, message=FALSE, warning=FALSE}
# 0) Font setup
library(showtext)
font_add_google("Merriweather", "merriweather")
showtext_auto()

# 1) Prep data: only Impersonal vs Personal, for step = -1 (solo) or drop step filter if you want all
library(dplyr)
library(forcats)
df_type <- greene_df %>%
  filter(type %in% c("Impersonal", "Personal")) %>%
  mutate(
    type_label = factor(type, levels = c("Impersonal", "Personal"))
  )

# 2) Compute means
means_df <- df_type %>%
  group_by(model_short, type_label) %>%
  summarise(mean_val = mean(opinion, na.rm = TRUE), .groups = "drop")

# 3) Palette
library(viridis)
pal_type <- viridis::viridis(2, option = "C", begin = 0.3, end = 0.7)
names(pal_type) <- c("Impersonal", "Personal")

# 4) Plot
library(ggplot2)

ggplot(df_type, aes(x = type_label, y = opinion, fill = type_label)) +
  geom_violin(trim = FALSE, alpha = 0.6) +
  # mean points
  geom_point(
    data = means_df,
    aes(y = mean_val),
    color = "white", fill = "red", shape = 21, size = 3
  ) +
  # mean labels
  geom_label(
    data = means_df,
    aes(y = mean_val, label = sprintf("%.2f", mean_val)),
    family     = "merriweather",
    fontface   = "bold",
    size       = 3,
    fill       = "lightyellow",
    color      = "black",
    label.size = 0.4,
    label.r    = unit(0.15, "lines"),
    vjust      = -0.8
  ) +
  facet_wrap(~ model_short, nrow = 1, scales = "free_x") +
  scale_fill_manual(values = pal_type, guide = "none") +
  labs(
    title    = "Opinion Distributions: Impersonal vs. Personal",
    subtitle = "Personal dilemmas elicit lower utilitarian scores",
    x        = "Dilemma Type",
    y        = "Opinion Rating (1–7)"
  ) +
  theme_minimal(base_family = "merriweather", base_size = 14) +
  theme(
    plot.title       = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle    = element_text(size = 12, hjust = 0.5, margin = margin(b = 8)),
    strip.text       = element_text(face = "bold", size = 8),
    axis.text.x      = element_text(size = 6),
    axis.title.y     = element_text(size = 10),
    panel.grid.minor = element_blank(),
    legend.position  = "none",
    plot.margin      = margin(15, 15, 15, 15)
  ) +
  scale_y_continuous(limits = c(1, 7), breaks = 1:7)

```



```{r mean-op-boxplot-stat4n1,cache=TRUE,echo=FALSE, message=FALSE, warning=FALSE}
diff_df2 <- greene_df %>%
  filter(ob=="n") %>%
  group_by(type,example_index,model_short) %>%
  summarise(mean_op = mean(opinion, na.rm = TRUE), .groups = "drop")

library(ggstatsplot)

grouped_ggbetweenstats(
  data          = filter(diff_df2, ),
  x                     = type,         
  y                     = mean_op,
  plot.type             = "box",           # box plots only
  type                  = "Parametric",     
  pairwise.comparisons  = TRUE,           # no p-values
  messages              = FALSE,
  effsize.type	= "eta",
  bf.message= FALSE,
  results.subtitle= FALSE,
  xlab =  "Group Size",
  pairwise.display = "significant",ylab = "Utilitarian Score",
centrality.plotting = TRUE,
  centrality.point.args = list(size = 5, color = "darkred"),
  centrality.label.args = list(size = 3, nudge_x = 0.4, segment.linetype = 4),
  centrality.path = TRUE,
  centrality.path.args = list(linewidth = 2, color = "red", alpha = 0.5),
  point.args = list(size = 4, alpha = 0.3, na.rm = TRUE),
  point.path = TRUE,
  point.path.args = list(alpha = 0.5, linetype = "dashed"),
  boxplot.args = list(width = 0.2, alpha = 0.4, na.rm = TRUE),
  violin.args = list(width = 0.0, alpha = 0.5, na.rm = TRUE),
  ggsignif.args = list(textsize = 2, tip_length = 0.01, na.rm = TRUE),
  ggtheme = ggplot2::theme_minimal(base_size = 12),
  package = "RColorBrewer",
  palette = "Set1",
grouping.var = model_short,
  ggplot.component = NULL)

```


## Utlitarian Boost Across Models
Plot From Solo to Group: Utilitarian boost across models (we saw that these are all significant)
```{r type-clmmplot, cache=TRUE, message=FALSE, warning=FALSE}
# 0) Font setup
font_add_google("Merriweather", "merriweather")
showtext_auto()

# 1) Prep data
df_violin <- greene_df %>%
  filter(step %in% c(-1, 7), type == "Personal") %>%
  mutate(
    step_label  = factor(ifelse(step == -1, "Solo", "Group"),
                         levels = c("Solo", "Group")),
    model_short = factor(model_short)
  )

# 2) Compute means
means_df <- df_violin %>%
  group_by(model_short, step_label) %>%
  summarise(mean_val = mean(opinion, na.rm = TRUE), .groups = "drop")

means_wide <- means_df %>%
  pivot_wider(names_from = step_label, values_from = mean_val)

# 3) Palette with transparency
library(scales)
pal <- c(
  Solo  = alpha("#2c7bb6", 0.1),
  Group = alpha("darkgreen", 0.1)
)

# 4) Plot
ggplot(df_violin, aes(step_label, opinion, fill = step_label)) +
  geom_violin(trim = FALSE, alpha = 0.6, color = NA) +
  geom_boxplot(
    width = 0.12, outlier.shape = NA,
    fill = "white", color = "black", alpha = 0.1
  ) +
  geom_jitter(
    aes(color = step_label),
    width = 0.08, size = 1.5,
    alpha = 0.3, show.legend = FALSE
  ) +
  scale_color_manual(values = pal) +
  # connecting lines
  geom_segment(
    data = means_wide, inherit.aes = FALSE,
    aes(x = 1, xend = 2, y = Solo, yend = Group),
    color = "black", size = 0.6
  ) +
  # red point for means
  geom_point(
    data = means_df, inherit.aes = FALSE,
    aes(x = step_label, y = mean_val),
    color = "red", size = 5
  ) +
  # callout boxes
  geom_label(
    data = means_df, inherit.aes = FALSE,
    aes(
      x = as.numeric(step_label) + ifelse(step_label == "Solo", -0.15, 0.15),
      y = mean_val + 0.3,
      label = sprintf("%.2f", mean_val)
    ),
    family     = "merriweather",
    fontface   = "bold",
    fill       = "lightyellow",
    color      = "black",
    label.size = 0.4,
    label.r    = unit(0.15, "lines"),
    size       = 3.5
  ) +
  facet_wrap(~ model_short, nrow = 1) +
  scale_fill_manual(values = pal) +
  labs(
    title    = "Solo vs. Group in Personal Dilemmas",
    subtitle = "Utilitarian Boost by Model",
    x        = NULL,
    y        = "Utilitarian Score"
  ) +
  theme_light(base_family = "merriweather", base_size = 13) +
  theme(
    plot.title        = element_text(size = 18, face = "bold", hjust = 0.5),
    plot.subtitle     = element_text(size = 14, hjust = 0.5, margin = margin(b = 10)),
    strip.text        = element_text(size = 12, face = "bold", color = "black"),
    axis.text.x       = element_text(size = 11),
    axis.title.y      = element_text(size = 12),
    panel.grid.major  = element_line(color = "gray"),
    panel.grid.minor  = element_blank(),
    legend.position   = "none",
    plot.margin       = margin(20, 20, 20, 20)
  )
```


## Sanity Check: Agent Name 

We check whether the agent name would lead to higher/lower Utilitarian Score. 

We do this for group size 2 (With two agent)
```{r type-clmmplot2, cache=TRUE, message=FALSE, warning=FALSE, results="hide"}
reflection_moral_g=filter(reflection_moral, ob=="nn")
# Fit cumulative link mixed model (CLMM) with random intercepts for example_index
#    and a random slope of 'step' by example_index:model to mirror (model | example_index)
typellm_clmm <- clmm(
  opinion ~ name + (rep|item)  ,
  data = reflection_moral_g,
  Hess = TRUE
)

# 3. Summarize the fitted model
summary(typellm_clmm)
```

We also do this for group size 3 (With three agents).

```{r typellm-clmm, cache=TRUE, message=FALSE, warning=FALSE,results='hide'}
reflection_moral_g=filter(reflection_moral,type=="Personal" & ob=="nnn")
# Fit cumulative link mixed model (CLMM) with random intercepts for example_index
#    and a random slope of 'step' by example_index:model to mirror (model | example_index)
typellm_clmm <- clmm(
  opinion ~ name + (rep|item)  ,
  data = reflection_moral_g,
  Hess = TRUE
)
# 4. Obtain pairwise estimated marginal means (on the probability scale)
emmstype <- emmeans(
  typellm_clmm,
  pairwise ~ name,
  type = "response"
)
```

```{r personal-vs-impersonal-same-model12, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
library(dplyr)
library(knitr)

#  Get contrasts as a data frame
contrast_df <- as.data.frame(summary(emmstype$contrasts))

kable(contrast_df, digits = 3, caption = "Estimated Marginal Means Contrasts (Tukey-adjusted)")

```


No difference is observed for Agent name (agent 1,2,3). No difference between agents name at stage 7  or Triads.


## LLM Consistency Checks

We want to check the effect of repetition on the the agents responses across all models. We include `rep` to model consistency. 

```{r type-clmmd, cache=TRUE, message=FALSE, warning=FALSE}
reflection_moral_s=filter(reflection_moral,type=="Personal" & ob=="nnn")
# Fit cumulative link mixed model (CLMM) with random intercepts for example_index
#    and a random slope of 'step' by example_index:model to mirror (1 | example_index)
typellm_clmm <- clmm(
  opinion ~ rep + (1|item)  ,
  data = reflection_moral_g,
  Hess = TRUE
)
# 3. Summarize the fitted model
summary(typellm_clmm)
```


```{r type-clmmda, cache=TRUE, message=FALSE, warning=FALSE}
reflection_moral_pb=filter(reflection_moral,type=="Personal" & step==7)
# Fit cumulative link mixed model (CLMM) with random intercepts for example_index
#    and a random slope of 'step' by example_index:model to mirror (model | example_index)
model_clmm <- clmm(
  opinion ~ rep +
    (1| item),
  data = reflection_moral,
  Hess = TRUE
)
```

No effect of rep is obsereved. 

## Item-based analysis

Here we check how different Items would lead to different utilitarian boost. We plot this change across different types. 


```{r mean-op-boxplot-stat21,cache=TRUE,echo=FALSE, message=FALSE, warning=TRUE}
diff_df2 <- greene_df %>%
  filter() %>%
  group_by(type,example_index,stepf) %>%
  summarise(mean_op = mean(opinion, na.rm = TRUE), .groups = "drop")

library(ggstatsplot)

ggwithinstats(
  data          = filter(diff_df2, type=="Personal" ),
  x                     = stepf,         
  y                     = mean_op,
  plot.type             = "box",           # box plots only
  type                  = "Parametric",     
  pairwise.comparisons  = TRUE,           # no p-values
  messages              = FALSE,
  effsize.type	= "eta",
  bf.message= TRUE,
  results.subtitle= TRUE,
  xlab =  "Group Size",
  pairwise.display = "significant",ylab = "Utilitarian Score",
  caption = "",
title = "Utilitarian Baseline Across Types (Single-Agent)",
subtitle = "",
centrality.plotting = TRUE,
  centrality.point.args = list(size = 5, color = "darkred"),
  centrality.label.args = list(size = 3, nudge_x = 0.4, segment.linetype = 4),
  centrality.path = TRUE,
  centrality.path.args = list(linewidth = 2, color = "red", alpha = 0.5),
  point.args = list(size = 4, alpha = 0.3, na.rm = TRUE),
  point.path = TRUE,
  point.path.args = list(alpha = 0.5, linetype = "dashed"),
  boxplot.args = list(width = 0.2, alpha = 0.4, na.rm = TRUE),
  violin.args = list(width = 0.0, alpha = 0.5, na.rm = TRUE),
  ggsignif.args = list(textsize = 2, tip_length = 0.01, na.rm = TRUE),
  ggtheme = ggplot2::theme_minimal(base_size = 12),
  package = "RColorBrewer",
  palette = "Set2",
  ggplot.component = NULL)

```


First we create a lookup table of the scenarios 
```{r type-clmmdbb, cache=TRUE, message=FALSE, warning=FALSE}
dilemma_lookup <- data.frame(
  example_index = c(21:39, 40:64),
  label = c(
    paste0(1:19, c(
      "Standard Trolley", "Standard Fumes", "Donation", "Transplant (Impersonal)", "Hospital Swap",
      "Minefield", "Footbridge Switch", "Bomb Shelter", "Back Alley", "Dog Dilemma",
      "Accidental Bump", "Oven", "Journalist", "Crash", "Sinkhole",
      "Missing Bridge", "Driver Switch", "Emergency Plan", "Eyes"
    )),
    paste0(1:24, c(
      "Transplant", "Footbridge", "Country Road", "Boss", "Lifeboat",
      "Hard Times", "Smother for Dollars", "Safari", "Crying Baby", "Plane Crash",
      "Hired Rapist", "Grandson", "Infanticide", "Preventing the Spread", "Modified Lifeboat",
      "Modified Preventing the Spread", "Modified Safari", "Modified Bomb", "Submarine",
      "Lawrence of Arabia", "Sophie’s Choice", "Sacrifice", "Vitamins", "Vaccine Test", "Euthanasia"
    ))
  )
)

dilemma_lookup$example_index=as.factor(dilemma_lookup$example_index)

```

```{r type-clmmcc, cache=TRUE, message=FALSE, warning=FALSE}
combined_dataset$example_index=as.factor(combined_dataset$example_index)
# Step 1: Compute difference scores and SD
df_diff_all <- combined_dataset %>%
  filter(dataset=="greene") %>%
  group_by(example_index, rep ,model,step,type) %>%
  summarise(mean_opinion = mean(opinion, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(
    names_from = step,
    values_from = mean_opinion,
    names_prefix = "step_"
  ) %>%
  filter(!is.na(`step_-1`) & !is.na(`step_7`)) %>%
  mutate(
    diff     = `step_7` - `step_-1`,
    abs_diff = abs(diff)
  ) %>%
  group_by(example_index,type) %>%
  summarise(
    mean_diff = mean(diff, na.rm = TRUE),
    sd_diff   = sd(diff, na.rm = TRUE),
    n         = n(),
    se_diff   = sd_diff / sqrt(n),
    .groups   = "drop"
  ) %>%
  arrange(desc(mean_diff)) %>%
  mutate(example_index = factor(example_index, levels = example_index))

df_diff_allp=filter(df_diff_all,type=="Personal")
# Step 2: Plot with error bars
ggplot(df_diff_allp, aes(x = example_index, y = mean_diff)) +
  geom_col(fill = "#66c2a5", width = 0.7) +
  geom_errorbar(aes(ymin = mean_diff - se_diff, ymax = mean_diff + se_diff),
                width = 0.2, color = "black") +
  geom_text(aes(label = sprintf("%.2f", mean_diff)),
            vjust = -0.5, size = 3) +
  labs(
    x     = "Example Index (sorted by Group – Solo)",
    y     = "Opinion Change (Mean ± SE)",
    title = "Opinion Change (Group → Solo) for Personal Scnearios"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x  = element_text(angle = 45, hjust = 1),
    plot.title   = element_text(hjust = 0.5, face = "bold")
  )
```


```{r type-clmmtt, cache=TRUE, message=FALSE, warning=FALSE}
dilemma_lookup$example_index=as.factor(dilemma_lookup$example_index)

df_diff_all$example_index=as.factor(df_diff_all$example_index)
df_plot <- df_diff_all %>%
  left_join(dilemma_lookup, by = "example_index") %>%
  arrange(desc(mean_diff)) %>%
  mutate(label = factor(example_index, levels = example_index))

# Plot
ggplot(df_plot, aes(x = label, y = mean_diff, fill = type)) +
  geom_col(width = 0.7) +
  geom_errorbar(aes(ymin = mean_diff - se_diff, ymax = mean_diff + se_diff),
                width = 0.2, color = "black") +
  geom_text(aes(label = sprintf("%.2f", mean_diff)), vjust = -0.5, size = 3) +
  labs(
    x = "Dilemma (Ordered by Opinion Change)",
    y = "Opinion Change (Mean ± SE)",
    title = "Opinion Change (Group → Solo) by Dilemma Type"
  ) +
  scale_fill_manual(values = c("Personal" = "#66c2a5", "Impersonal" = "#fc8d62")) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.title = element_blank()
  )

```



## Balanced Sample checks

Here we include the rep<16 so we have a balanced sample and check the robustness of the result

```{r BS, echo=TRUE, cachE=TRUE}
reflection_moralrep=filter(reflection_moral, rep<16)
model2rep <- clmm(
  opinion ~ Group *type  + (rep | item),
  data  = reflection_moralrep,
  Hess  = TRUE
)
```


Similar to the complete sample, we see the effect of Utilitraian boost in personal moral scenarios. 


```{r}
# 4. Obtain pairwise estimated marginal means (on the probability scale)
emms <- emmeans(
  model2rep,
  pairwise ~ Group * type ,
  type = "response"
)
emms
```



## Action vs. Ommision

### Action

We use the same model as before, but here we focus on the `type == "Action"` vs. `type == "Omission"`

```{r actio, echo=TRUE, cachE=TRUE}
reflection_moralrep=filter(combined_dataset, dataset=="keshmirian" & type=="Action")
reflection_moralrep$opinion=as.factor (reflection_moralrep$opinion)
model2rep <- clmm(
  opinion ~ stepf + (rep | item),
  data  = reflection_moralrep,
  Hess  = TRUE
)
```

We do not see any effect of Action. We investigate this effect for each models.

```{r action, echo=TRUE, cachE=TRUE}
emms <- emmeans(
  model2rep,
  pairwise ~ stepf ,
  type = "response"
)
emms
```

```{r repllaction, echo=TRUE, cachE=TRUE}
# List of models to compare
models_to_compare <- c("Gemma3", "GPT4.1", "Lamma3.3", "Qwen2.5", "Qwen3", "QWQ")

# Function to fit the model and extract contrasts
get_contrast <- function(m) {
  df_mod <- reflection_moralrep %>%
    filter(model_short == m)
  
  fit_mod <- clm(opinion ~ stepf, data = df_mod, Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ stepf, type = "response")
  as.data.frame(summary(em$contrasts)) %>%
    transmute(
      Model        = m,
      Contrast     = contrast,
      Estimate     = estimate,
      `Std. Error` = SE,
      `z value`    = z.ratio,
      `p value`    = p.value
    )
}
contrast_tbl <- bind_rows(lapply(models_to_compare, get_contrast))
# Filter to only the contrasts with p value below 0.05
significant_contrasts <- contrast_tbl %>%
  filter(`p value` < 0.05)

# Display them
significant_contrasts

```


### Omission 


We use the same model as before, but here we focus  `type == "Omission"`

```{r Omi, echo=TRUE, cachE=TRUE}
reflection_moralrep=filter(combined_dataset, dataset=="keshmirian" & type=="Omission")
reflection_moralrep$opinion=as.factor (reflection_moralrep$opinion)
model2rep <- clmm(
  opinion ~ stepf + (rep | item),
  data  = reflection_moralrep,
  Hess  = TRUE
)
```

We do not see any effect of Action. We investigate this effect for each models.

```{r repllmw, echo=TRUE, cachE=TRUE}
emms <- emmeans(
  model2rep,
  pairwise ~ stepf ,
  type = "response"
)
emms
```

```{r omi2, echo=TRUE, cachE=TRUE}
# List of models to compare
models_to_compare <- c("Gemma3", "GPT4.1", "Lamma3.3", "Qwen2.5", "Qwen3", "QWQ")

# Function to fit the model and extract contrasts
get_contrast <- function(m) {
  df_mod <- reflection_moralrep %>%
    filter(model_short == m)
  
  fit_mod <- clm(opinion ~ stepf, data = df_mod, Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ stepf, type = "response")
  as.data.frame(summary(em$contrasts)) %>%
    transmute(
      Model        = m,
      Contrast     = contrast,
      Estimate     = estimate,
      `Std. Error` = SE,
      `z value`    = z.ratio,
      `p value`    = p.value
    )
}
contrast_tbl <- bind_rows(lapply(models_to_compare, get_contrast))
# Filter to only the contrasts with p value below 0.05
significant_contrasts <- contrast_tbl %>%
  filter(`p value` < 0.05)

# Display them
significant_contrasts

```




# Factual Utilitarian Dilemma (dataset: Korner)

## Killing - Utlitarian


We use the same model as before, but here we focus  `type == "Killing–Util"`

```{r KU, echo=TRUE, cachE=TRUE}
reflection_moralrep=filter(combined_dataset, dataset=="korner" & type=="Killing–Util")
reflection_moralrep$opinion=as.factor (reflection_moralrep$opinion)
model2rep <- clmm(
  opinion ~ stepf + (rep | item),
  data  = reflection_moralrep,
  Hess  = TRUE
)

emms <- emmeans(
  model2rep,
  pairwise ~ stepf ,
  type = "response"
)
emms

models_to_compare <- c("Gemma3", "GPT4.1", "Lamma3.3", "Qwen2.5", "Qwen3", "QWQ")

# Function to fit the model and extract contrasts
get_contrast <- function(m) {
  df_mod <- reflection_moralrep %>%
    filter(model_short == m)
  
  fit_mod <- clm(opinion ~ stepf, data = df_mod, Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ stepf, type = "response")
  as.data.frame(summary(em$contrasts)) %>%
    transmute(
      Model        = m,
      Contrast     = contrast,
      Estimate     = estimate,
      `Std. Error` = SE,
      `z value`    = z.ratio,
      `p value`    = p.value
    )
}
contrast_tbl <- bind_rows(lapply(models_to_compare, get_contrast))
# Filter to only the contrasts with p value below 0.05
significant_contrasts <- contrast_tbl %>%
  filter(`p value` < 0.05)

# Display them
significant_contrasts
```


## Killing - Utlitarian
We use the same model as before, but here we focus  `type == "Killing–Util"`

```{r repllm, echo=TRUE, cachE=TRUE}
reflection_moralrep=filter(combined_dataset, dataset=="korner" & type=="Other–Util")
reflection_moralrep$opinion=as.factor (reflection_moralrep$opinion)
model2rep <- clmm(
  opinion ~ stepf + (rep | item),
  data  = reflection_moralrep,
  Hess  = TRUE
)

emms <- emmeans(
  model2rep,
  pairwise ~ stepf ,
  type = "response"
)
emms

models_to_compare <- c("Gemma3", "GPT4.1", "Lamma3.3", "Qwen2.5", "Qwen3", "QWQ")

# Function to fit the model and extract contrasts
get_contrast <- function(m) {
  df_mod <- reflection_moralrep %>%
    filter(model_short == m)
  
  fit_mod <- clm(opinion ~ stepf, data = df_mod, Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ stepf, type = "response")
  as.data.frame(summary(em$contrasts)) %>%
    transmute(
      Model        = m,
      Contrast     = contrast,
      Estimate     = estimate,
      `Std. Error` = SE,
      `z value`    = z.ratio,
      `p value`    = p.value
    )
}
contrast_tbl <- bind_rows(lapply(models_to_compare, get_contrast))
# Filter to only the contrasts with p value below 0.05
significant_contrasts <- contrast_tbl %>%
  filter(`p value` < 0.05)

# Display them
significant_contrasts
```

We use the same model as before, but here we focus  `type == "Other–Deon"`

## Other–Deontology

```{r OD, echo=TRUE, cachE=TRUE}
reflection_moralrep=filter(combined_dataset, dataset=="korner" & type=="Other–Deon")
reflection_moralrep$opinion=as.factor (reflection_moralrep$opinion)
model2rep <- clmm(
  opinion ~ stepf + (rep | item),
  data  = reflection_moralrep,
  Hess  = TRUE
)

emms <- emmeans(
  model2rep,
  pairwise ~ stepf ,
  type = "response"
)
emms

models_to_compare <- c("Gemma3", "GPT4.1", "Lamma3.3", "Qwen2.5", "Qwen3", "QWQ")

# Function to fit the model and extract contrasts
get_contrast <- function(m) {
  df_mod <- reflection_moralrep %>%
    filter(model_short == m)
  
  fit_mod <- clm(opinion ~ stepf, data = df_mod, Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ stepf, type = "response")
  as.data.frame(summary(em$contrasts)) %>%
    transmute(
      Model        = m,
      Contrast     = contrast,
      Estimate     = estimate,
      `Std. Error` = SE,
      `z value`    = z.ratio,
      `p value`    = p.value
    )
}
contrast_tbl <- bind_rows(lapply(models_to_compare, get_contrast))
# Filter to only the contrasts with p value below 0.05
significant_contrasts <- contrast_tbl %>%
  filter(`p value` < 0.05)

# Display them
significant_contrasts
```

We use the same model as before, but here we focus  `type == "Saving–Deon"`

## Saving–Deontology
```{r SD, echo=TRUE, cachE=TRUE}
reflection_moralrep=filter(combined_dataset, dataset=="korner" & type=="Saving–Deon")
reflection_moralrep$opinion=as.factor (reflection_moralrep$opinion)
model2rep <- clmm(
  opinion ~ stepf + (rep | item),
  data  = reflection_moralrep,
  Hess  = TRUE
)

emms <- emmeans(
  model2rep,
  pairwise ~ stepf ,
  type = "response"
)
emms

models_to_compare <- c("Gemma3", "GPT4.1", "Lamma3.3", "Qwen2.5", "Qwen3", "QWQ")

# Function to fit the model and extract contrasts
get_contrast <- function(m) {
  df_mod <- reflection_moralrep %>%
    filter(model_short == m)
  
  fit_mod <- clm(opinion ~ stepf, data = df_mod, Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ stepf, type = "response")
  as.data.frame(summary(em$contrasts)) %>%
    transmute(
      Model        = m,
      Contrast     = contrast,
      Estimate     = estimate,
      `Std. Error` = SE,
      `z value`    = z.ratio,
      `p value`    = p.value
    )
}
contrast_tbl <- bind_rows(lapply(models_to_compare, get_contrast))
# Filter to only the contrasts with p value below 0.05
significant_contrasts <- contrast_tbl %>%
  filter(`p value` < 0.05)

# Display them
significant_contrasts
```

# Oxford Utilitarian Scale (dataset: oxford)


## Instrumental Harm


```{r IH, echo=TRUE, cachE=TRUE}
reflection_moralrep=filter(combined_dataset, dataset=="oxford" & type=="Harm")
reflection_moralrep$opinion=as.factor (reflection_moralrep$opinion)
model2rep <- clmm(
  opinion ~ stepf + (rep | item),
  data  = reflection_moralrep,
  Hess  = TRUE
)

emms <- emmeans(
  model2rep,
  pairwise ~ stepf ,
  type = "response"
)
emms

models_to_compare <- c("Gemma3", "GPT4.1", "Lamma3.3", "Qwen2.5", "Qwen3", "QWQ")

# Function to fit the model and extract contrasts
get_contrast <- function(m) {
  df_mod <- reflection_moralrep %>%
    filter(model_short == m)
  
  fit_mod <- clm(opinion ~ stepf, data = df_mod, Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ stepf, type = "response")
  as.data.frame(summary(em$contrasts)) %>%
    transmute(
      Model        = m,
      Contrast     = contrast,
      Estimate     = estimate,
      `Std. Error` = SE,
      `z value`    = z.ratio,
      `p value`    = p.value
    )
}
contrast_tbl <- bind_rows(lapply(models_to_compare, get_contrast))
# Filter to only the contrasts with p value below 0.05
significant_contrasts <- contrast_tbl %>%
  filter(`p value` < 0.05)

# Display them
significant_contrasts
```

## Impartial Beneficence


```{r IB, echo=TRUE, cachE=TRUE}
reflection_moralrep=filter(combined_dataset, dataset=="oxford" & type=="Beneficence")
reflection_moralrep$opinion=as.factor (reflection_moralrep$opinion)
model2rep <- clmm(
  opinion ~ stepf + (rep | item),
  data  = reflection_moralrep,
  Hess  = TRUE
)

emms <- emmeans(
  model2rep,
  pairwise ~ stepf ,
  type = "response"
)
emms

models_to_compare <- c("Gemma3", "GPT4.1", "Lamma3.3", "Qwen2.5", "Qwen3", "QWQ")

# Function to fit the model and extract contrasts
get_contrast <- function(m) {
  df_mod <- reflection_moralrep %>%
    filter(model_short == m)
  
  fit_mod <- clm(opinion ~ stepf, data = df_mod, Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ stepf, type = "response")
  as.data.frame(summary(em$contrasts)) %>%
    transmute(
      Model        = m,
      Contrast     = contrast,
      Estimate     = estimate,
      `Std. Error` = SE,
      `z value`    = z.ratio,
      `p value`    = p.value
    )
}
contrast_tbl <- bind_rows(lapply(models_to_compare, get_contrast))
# Filter to only the contrasts with p value below 0.05
significant_contrasts <- contrast_tbl %>%
  filter(`p value` < 0.05)

# Display them
significant_contrasts
```


# CNI Utilitarian Dilemmas (dataset: CNI)


## Action–Incongruent


```{r AI, echo=TRUE, cachE=TRUE}
reflection_moralrep=filter(combined_dataset, dataset=="oxford" & type=="Beneficence")
reflection_moralrep$opinion=as.factor (reflection_moralrep$opinion)
model2rep <- clmm(
  opinion ~ stepf + (rep | item),
  data  = reflection_moralrep,
  Hess  = TRUE
)

emms <- emmeans(
  model2rep,
  pairwise ~ stepf ,
  type = "response"
)
emms

models_to_compare <- c("Gemma3", "GPT4.1", "Lamma3.3", "Qwen2.5", "Qwen3", "QWQ")

# Function to fit the model and extract contrasts
get_contrast <- function(m) {
  df_mod <- reflection_moralrep %>%
    filter(model_short == m)
  
  fit_mod <- clm(opinion ~ stepf, data = df_mod, Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ stepf, type = "response")
  as.data.frame(summary(em$contrasts)) %>%
    transmute(
      Model        = m,
      Contrast     = contrast,
      Estimate     = estimate,
      `Std. Error` = SE,
      `z value`    = z.ratio,
      `p value`    = p.value
    )
}
contrast_tbl <- bind_rows(lapply(models_to_compare, get_contrast))
# Filter to only the contrasts with p value below 0.05
significant_contrasts <- contrast_tbl %>%
  filter(`p value` < 0.05)

# Display them
significant_contrasts
```


## Omission–Incongruent 

```{r oI, echo=TRUE, cachE=TRUE}
reflection_moralrep=filter(combined_dataset, dataset=="oxford" & type=="Beneficence")
reflection_moralrep$opinion=as.factor (reflection_moralrep$opinion)
model2rep <- clmm(
  opinion ~ stepf + (rep | item),
  data  = reflection_moralrep,
  Hess  = TRUE
)

emms <- emmeans(
  model2rep,
  pairwise ~ stepf ,
  type = "response"
)
emms

models_to_compare <- c("Gemma3", "GPT4.1", "Lamma3.3", "Qwen2.5", "Qwen3", "QWQ")

# Function to fit the model and extract contrasts
get_contrast <- function(m) {
  df_mod <- reflection_moralrep %>%
    filter(model_short == m)
  
  fit_mod <- clm(opinion ~ stepf, data = df_mod, Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ stepf, type = "response")
  as.data.frame(summary(em$contrasts)) %>%
    transmute(
      Model        = m,
      Contrast     = contrast,
      Estimate     = estimate,
      `Std. Error` = SE,
      `z value`    = z.ratio,
      `p value`    = p.value
    )
}
contrast_tbl <- bind_rows(lapply(models_to_compare, get_contrast))
# Filter to only the contrasts with p value below 0.05
significant_contrasts <- contrast_tbl %>%
  filter(`p value` < 0.05)

# Display them
significant_contrasts
```


## Action–Congruent" 

```{r AC, echo=TRUE, cachE=TRUE}
reflection_moralrep=filter(combined_dataset, dataset=="oxford" & type=="Beneficence")
reflection_moralrep$opinion=as.factor (reflection_moralrep$opinion)
model2rep <- clmm(
  opinion ~ stepf + (rep | item),
  data  = reflection_moralrep,
  Hess  = TRUE
)

emms <- emmeans(
  model2rep,
  pairwise ~ stepf ,
  type = "response"
)
emms

models_to_compare <- c("Gemma3", "GPT4.1", "Lamma3.3", "Qwen2.5", "Qwen3", "QWQ")

# Function to fit the model and extract contrasts
get_contrast <- function(m) {
  df_mod <- reflection_moralrep %>%
    filter(model_short == m)
  
  fit_mod <- clm(opinion ~ stepf, data = df_mod, Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ stepf, type = "response")
  as.data.frame(summary(em$contrasts)) %>%
    transmute(
      Model        = m,
      Contrast     = contrast,
      Estimate     = estimate,
      `Std. Error` = SE,
      `z value`    = z.ratio,
      `p value`    = p.value
    )
}
contrast_tbl <- bind_rows(lapply(models_to_compare, get_contrast))
# Filter to only the contrasts with p value below 0.05
significant_contrasts <- contrast_tbl %>%
  filter(`p value` < 0.05)

# Display them
significant_contrasts
```


## Omission–Congruent"  
  
```{r OC, echo=TRUE, cachE=TRUE}
reflection_moralrep=filter(combined_dataset, dataset=="oxford" & type=="Beneficence")
reflection_moralrep$opinion=as.factor (reflection_moralrep$opinion)
model2rep <- clmm(
  opinion ~ stepf + (rep | item),
  data  = reflection_moralrep,
  Hess  = TRUE
)

emms <- emmeans(
  model2rep,
  pairwise ~ stepf ,
  type = "response"
)
emms

models_to_compare <- c("Gemma3", "GPT4.1", "Lamma3.3", "Qwen2.5", "Qwen3", "QWQ")

# Function to fit the model and extract contrasts
get_contrast <- function(m) {
  df_mod <- reflection_moralrep %>%
    filter(model_short == m)
  
  fit_mod <- clm(opinion ~ stepf, data = df_mod, Hess = TRUE)
  em <- emmeans(fit_mod, pairwise ~ stepf, type = "response")
  as.data.frame(summary(em$contrasts)) %>%
    transmute(
      Model        = m,
      Contrast     = contrast,
      Estimate     = estimate,
      `Std. Error` = SE,
      `z value`    = z.ratio,
      `p value`    = p.value
    )
}
contrast_tbl <- bind_rows(lapply(models_to_compare, get_contrast))
# Filter to only the contrasts with p value below 0.05
significant_contrasts <- contrast_tbl %>%
  filter(`p value` < 0.05)

# Display them
significant_contrasts
```
