---
title: "ManyLLMs: Data Cleaning"
author: "Authors: XXX"
date: "2025-05-21"
output:
  pdf_document:
    toc: true
    number_sections: true
    latex_engine: xelatex
---

<!-- Custom style: works only for HTML output -->
<style type="text/css">
p.author-note {
  font-size: 10pt;
  font-style: italic;
  color: #444;
  margin-top: -1em;
  margin-bottom: 2em;
}
</style>





# Overview


<div class="author-note">
This script accompanies the analyses presented in the manuscript.
Two versions are provided:
</div>

- The `.Rmd` file can be opened and edited in **RStudio**.  
- A rendered **PDF** is available for reference, generated by knitting this file.

---

We provide full R code to ensure the analyses are transparent and reproducible.  
Where necessary, code is explained or unpacked in-text. This document includes:

1. Visualizations of key variable distributions (used in main and supplementary materials)  
2. Justifications for major analysis decisions

_Last updated: **21 May 2025**_



```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  echo = TRUE
)
library(dplyr)
library(forcats)
library(stringr)
library(ggplot2)
library(scales)
library(viridis)
library(patchwork)
library(stringr)
library(knitr)
library(tibble)
```


# Step 1: Data Preparation

In this step, we prepare the environment and load data from three experimental conditions.

First, we **clear the workspace** using `rm(list = ls(all.names = TRUE))` and load the necessary R libraries such as `dplyr`, `ggplot2`, `forcats`, and `patchwork` for data manipulation and visualization.

We then read in three `.csv` files corresponding to the three group conditions:

- *Single-agent condition*: stored in the variable `r "single"`
- *Pair-agent condition*: stored in `r "nob_2"`
- *Triad-agent condition*: stored in `r "nob_3"`

Each dataset receives an additional column, `r "experiment"`, which labels its group condition.

These datasets are then **merged** using `bind_rows()` into one unified data frame:  
**`r "combined_df"`**

This combined dataset will be filtered, cleaned, and analyzed in the next steps.


```{r}
rm(list = ls(all.names = TRUE))
```


```{r}
setwd("/Users/nita/Downloads")

# 1. Read each file and add an experiment label

# Single Agent (Solo)
single   <- read.csv("single_no_message_f.csv",   header = TRUE, stringsAsFactors = TRUE) %>%
  mutate(experiment = "Single") 

#Pairs (groups)
nob_2    <- read.csv("nob_2_no_message_f.csv",   header = TRUE, stringsAsFactors = TRUE) %>%
  mutate(experiment = "NoB_2")

#Triad (groups)
nob_3    <- read.csv("nob_3_no_message_f.csv",     header = TRUE, stringsAsFactors = TRUE) %>%
  mutate(experiment = "NoB_3")

# 3. Combine
combined_df <- bind_rows(single,nob_2,nob_3)
```

We remove the column `r "onboarding"` from `r "combined_df"` as it is *not needed* for further analysis.

Next, we **filter the dataset** to retain only the observations from step `r -1` (pre-condition) and step `r 7` (post-condition), which correspond to the *single* and *group* conditions. After filtering, we apply `droplevels()` to clean up unused factor levels.

To focus our analysis on relevant moral judgments, we also remove any **non-moral scenarios** by filtering out rows where the `r "type"` column is `"Non-Moral"`.

Finally, we simplify verbose model names by recoding them using `fct_recode()` and storing the cleaned names in a new column:  
**`r "model_short"`**

This step ensures the dataset is concise and easier to interpret in subsequent analysis and visualization.


```{r}
# 4.  Drop the unwanted columns
combined_df <- combined_df %>% select(-onboarding)


# 5. Select only Step -1 and 7 opinion (Single vs. Groups) no discussion
combined_df= filter(combined_df, step == -1 | step ==7) %>% droplevels()

# 6. delete the non-moral scenarios 
combined_df= filter(combined_df, type!="Non-Moral") %>% droplevels
```

Next, we **standardize** the dataset names by recoding `"oxford_utilitarianism_scale"` to a shorter label: `"oxford"`.

We also recode values in the `type` column to group similar scenario categories under clearer, more interpretable labels — for example:  
`"Factual–Killing–Utilitarian"` becomes `"Killing–Util"` and  
`"Factual–Saving–Deontological"` becomes `"Saving–Deon"`.

To enhance readability, we also **shorten long model names** by assigning simplified names to a new variable: `model_short`.

For items belonging to the `"CNI"` dataset, we create a new column `cni_type` by mapping each `example_index` to a predefined list of CNI scenario codes.

From these codes, we extract meaningful suffixes such as `"acinc"`, `"ininc"`, `"incon"`, or `"accon"` and update the `type` column accordingly.

This finalizes the dataset’s semantic structure, aligning it with the categories used throughout the manuscript.

```{r}
# 7. Make llm names shorter

combined_df <- combined_df %>%
  mutate(
    model_short = fct_recode(model,
      "GPT4.1"   = "gpt-4.1",
      "Lamma3.3" = "llama3.3",
      "Qwen2.5"= "qwen2.5:32b-instruct",
      "Qwen3"  = "qwen3:32b",
      "QWQ"    = "qwq",
      "Gemma3" = "gemma3:27b"
    )
)


# 7 changing the levles of dataset 
combined_df <- combined_df %>%
  mutate(dataset = recode(dataset, "oxford_utilitarianism_scale" = "oxford"))
# 8. changing the type: 

library(forcats)

combined_df$type <- fct_recode(combined_df$type,
  "Action"        = "action",
  "Killing–Util"  = "Factual–Killing–Utilitarian",
  "Other–Deon"    = "Factual–Other–Deontological",
  "Other–Util"    = "Factual–Other–Utilitarian",
  "Saving–Deon"   = "Factual–Saving–Deontological",
  "Beneficence"   = "Impartial Beneficence",
  "Harm"          = "Instrumental Harm",
  "Omission"      = "omission",
  "CNI"           = "N/A"
  # "Impersonal" and "Personal" remain unchanged
)


#fix cni types
cni_labels <- c(
  "d1acinc", "d6ininc", "d5ininc", "d4incon", "d3ininc", "d2ininc",
  "d1ininc", "d2acinc", "d6accon", "d5accon", "d4acinc", "d3incon",
  "d2incon", "d6incon", "d1incon", "d3accon", "d5acinc", "d4accon",
  "d2accon", "d6acinc", "d3acinc", "d4ininc", "d5incon", "d1accon"
)
combined_df <- combined_df %>%
  mutate(example_index = as.character(example_index)) %>%
  mutate(
    cni_type = case_when(
      type == "CNI" ~ factor(
        example_index,
        levels = as.character(seq_along(cni_labels)),
        labels = cni_labels
      ),
      TRUE ~ NA_character_
    )
  )

combined_df <- combined_df %>%
  mutate(
    type = ifelse(
      type == "CNI",
      str_extract(cni_type, "acinc|ininc|incon|accon"),
      as.character(type)
    ),
    type = factor(type, levels = c("acinc", "ininc", "incon", "accon",
                                   "Action", "Killing–Util", "Other–Deon", 
                                   "Other–Util", "Saving–Deon", "Beneficence", 
                                   "Harm", "Impersonal", "Personal", "Omission"))
  )
```

Finally, we address a known **misalignment** in the `r "greene"` dataset:  
any `r "example_index"` values greater than `r 41` are off by one.  
We correct this by subtracting `r 1` from those values.

The corrected `example_index` column is then converted into a factor named `r "item"` to prepare it for grouped analysis.

As a sanity check, we verify that **all expected item indices** from `r 1` to `r max(combined_df$example_index)` are present.

This concludes the *initial data preparation step*, ensuring that the dataset is **clean**, **consistent**, and fully ready for downstream analysis.


```{r}
#Fixiing issues with Green

library(dplyr)
combined_df$example_index=as.numeric(combined_df$example_index)
# Step 1: Apply correction ONLY to greene where index > 41
combined_df <- combined_df %>%
  mutate(
    example_index = case_when(
      dataset == "greene" & example_index > 41 ~ example_index - 1,
      TRUE ~ example_index
    )
  )
combined_df$item=as.factor(combined_df$example_index)
combined_df$stepf=as.factor(combined_df$step)



expected_indices <- 1:max(combined_df$example_index)
missing_indices <- setdiff(expected_indices, combined_df$example_index)

expected_indices <- 1:max(combined_df$example_index)
missing_indices <- setdiff(expected_indices, combined_df$example_index)
if (length(missing_indices) == 0) {
  cat("✅ All indices from 1 to", max(combined_df$example_index), "are present.\n")
} else {
  cat("❌ Missing indices:", paste(missing_indices, collapse = ", "), "\n")
}
```


There are `r length(missing_indices)` missing indices between `r min(expected_indices)` and `r max(expected_indices)`.


# Step 2: Cleaning and Formatting

Note that the dataset has already been *pre-processed*, as described in the **Supplementary Materials**.  
Here, we check for any **additional mismatches or missing entries** that may affect downstream analysis.

In this step, we clean the `combined_df` dataset by identifying and removing problematic values in the `opinion` column (also referred to as the *utilitarian score*).


```{r}
# 1. Convert to character and numeric
opinion_char <- as.character(combined_df$opinion)
opinion_num  <- suppressWarnings(as.numeric(opinion_char))

# 2. Define conditions
is_na         <- is.na(combined_df$opinion)
is_text       <- !str_detect(opinion_char, "^-?\\d+(\\.\\d+)?$")
out_of_range  <- !is.na(opinion_num) & (opinion_num < 1 | opinion_num > 7)
invalid_all   <- is_na | is_text | out_of_range
```


We begin by converting opinion into two formats:  
- as character: opinion_char  
- as numeric: opinion_num


```{r}
# Calculate total invalid entries
total_rows    <- nrow(combined_df)
total_missing <- sum(invalid_all, na.rm = TRUE)
```


We then define several **invalid conditions**:

- is_na — detects *missing values*  
- is_text — identifies *non-numeric entries* (e.g., text or symbols)  
- out_of_range — flags numeric values *outside the valid 1–7 Likert scale*

All three are combined into a master logical flag: **invalid_all**.  
We then compute and print their counts.

The **total number of invalid entries** is stored in total_missing, which equals r total_missing observations.  
These include any missing, misformatted, or out-of-range values in the data.

The **total number of invalid entries** is stored in `total_missing`, which equals `r total_missing` observations.  
These include any missing, misformatted, or out-of-range values in the data.


We print a concise summary of issues found in the `opinion` column of `combined_df`:

- **NA values** are missing responses.
- **Text values** are improperly formatted (e.g., strings instead of numbers).
- **Out-of-range values** fall outside the valid Likert scale (1–7).
- All of these are grouped under **invalid entries** and counted as a share of the total.

This summary helps assess the integrity of the data before applying filters or imputation.

```{r}
# 3. Print counts
# Summary counts for opinion cleaning
total_rows    <- nrow(combined_df)
na_count      <- sum(is_na)
text_count    <- sum(is_text)
range_count   <- sum(out_of_range)
invalid_total <- sum(invalid_all)
```


```{r, echo=FALSE, message=FALSE, results='asis'}
library(tibble)
library(knitr)

# Create a summary table
data_quality_summary <- tibble::tibble(
  Metric = c("Total rows",
             "NA values",
             "Non-numeric (text)",
             "Out-of-range (not 1–7)",
             "Total invalid entries"),
  Count = c(total_rows, na_count, text_count, range_count, invalid_total)
)

# Display the table
knitr::kable(data_quality_summary, caption = "Table: Data Quality Summary")
```


```{r}
# 4. Show the percentage of missing values

total_rows    <- nrow(combined_df)
total_missing <- sum(invalid_all, na.rm = TRUE)
cat("Overall missing:", total_missing, "of", total_rows,
    sprintf("(%.1f%%)\n", 100 * total_missing / total_rows))
```

We find that `r total_missing` responses are marked as invalid.
To quantify the extent of invalid data, we calculate:

- `r total_rows`: the total number of entries in the dataset  
- `r total_missing`: the total number of entries that are missing, misformatted, or out of range

```{r}
x =100 * total_missing / total_rows
print(round(x, 2))
```

The percentage of invalid responses is `r round(x, 2)`%.

We remove these invalid rows, creating a cleaned version called `combined_df_clean`.

```{r}
# 1. Remove invalid rows
combined_df_clean <- combined_df[!invalid_all, , drop = FALSE]

# 2. Convert remaining opinion values to numeric
combined_df_clean$opinion <- as.numeric(as.character(combined_df_clean$opinion))
```

Then, we handle decimal values. We check how many `opinion` values are not whole numbers using `non_integer_before`. We then round them and recheck using `non_integer_after` to ensure all opinions are integers.

Finally, we:
- Convert character columns to factors
- Coerce `opinion` and `example_index` to integer
- Confirm structure with `glimpse(combined_df_clean)`

This step ensures that `combined_df_clean` is numerically valid, integer-formatted, and properly typed, ready for downstream statistical modeling.


```{r}
# Count non-integer opinion values before rounding
non_integer_before <- sum(combined_df_clean$opinion %% 1 != 0, na.rm = TRUE)

# Round to nearest integer
combined_df_clean$opinion <- round(combined_df_clean$opinion)

# Count after rounding
non_integer_after <- sum(combined_df_clean$opinion %% 1 != 0, na.rm = TRUE)

# For inline reporting
total_rows_clean <- nrow(combined_df_clean)
percent_nonint_before <- 100 * non_integer_before / total_rows_clean
```


Among the `r total_rows_clean` valid responses, we found that `r non_integer_before` values  
(`r round(percent_nonint_before, 2)`%) were **non-integer** prior to rounding.  

After applying `round()`, the number of non-integer values is `r non_integer_after`.

This ensures all `opinion` values are now stored as whole numbers between 1 and 7.


```{r}
# 6 Convert  Convert character columns to factors
combined_df_clean <- combined_df_clean %>% mutate_if(is.character, as.factor)

combined_df_clean$opinion= as.integer(combined_df_clean$opinion)  
combined_df_clean$example_index=as.integer(combined_df_clean$example_index)

glimpse(combined_df_clean)

```


# Step 3: Visualizing Invalid Values

### Step X: Visual Inspection of Invalid Values

In this step, we **visually inspect the frequency and distribution** of invalid entries in the `opinion` column across different experimental groupings.

We begin by defining four types of invalid entries:

- `is_na`: *missing values* (`NA`)
- `is_text`: *non-numeric values* (e.g., letters or symbols)
- `out_of_range`: *numeric values outside the valid 1–7 Likert range*
- `non_integer`: *numeric values that are not integers* (e.g., `3.5`)

These flags are appended as new logical columns to the dataset `combined_df`.

```{r}


# 1. Extract opinion column and parse
opinion_char <- as.character(combined_df$opinion)
opinion_num  <- suppressWarnings(as.numeric(opinion_char))

# 2. Define invalid types
is_na        <- is.na(combined_df$opinion)
is_text      <- !str_detect(opinion_char, "^-?\\d+(\\.\\d+)?$")
out_of_range <- !is.na(opinion_num) & (opinion_num < 1 | opinion_num > 7)
non_integer  <- opinion_num %% 1 != 0 & !is.na(opinion_num)
invalid_all  <- is_na | is_text | out_of_range

# Add as new columns for grouped use
combined_df <- combined_df %>%
  mutate(
    is_na        = is_na,
    is_text      = is_text,
    out_of_range = out_of_range,
    non_integer  = non_integer
  )

# 3. Helper: summarise invalid % per group
get_invalid_summary <- function(df, group_var) {
  df %>%
    group_by(.data[[group_var]]) %>%
    summarise(
      pct_na        = 100 * mean(is_na),
      pct_text      = 100 * mean(is_text),
      pct_range     = 100 * mean(out_of_range),
      pct_nonint    = 100 * mean(non_integer),
      .groups = "drop"
    ) %>%
    rename(group = all_of(group_var))
}

# 4. Compute for each group
df_model  <- get_invalid_summary(combined_df, "model_short")
df_type   <- get_invalid_summary(combined_df, "dataset")
df_group  <- get_invalid_summary(combined_df, "group_size")

# 5. Helper: plot each type
plot_invalid <- function(df, var, fill_color, title_text) {
  ggplot(df, aes(x = reorder(group, !!sym(var)), y = !!sym(var))) +
    geom_col(fill = fill_color) +
    geom_text(aes(label = sprintf("%.1f%%", !!sym(var))), vjust = -0.5, size = 3) +
    scale_y_continuous(expand = expansion(c(0, 0.1)), labels = percent_format(scale = 1)) +
    labs(x = "Group", y = "%", title = title_text) +
    theme_minimal(base_size = 11) +
    theme(plot.title = element_text(face = "bold", hjust = 0.5))
}

# 6. Create plot grids
p1 <- plot_invalid(df_model, "pct_na", "#66c2a5", "NA by Model")
p2 <- plot_invalid(df_model, "pct_text", "#fc8d62", "Text Errors by Model")
p3 <- plot_invalid(df_model, "pct_range", "#8da0cb", "Out-of-Range by Model")
p4 <- plot_invalid(df_model, "pct_nonint", "#e78ac3", "Non-Integer by Model")

p5 <- plot_invalid(df_type, "pct_na", "#66c2a5", "NA by Dataset")
p6 <- plot_invalid(df_type, "pct_text", "#fc8d62", "Text Errors by Dataset")
p7 <- plot_invalid(df_type, "pct_range", "#8da0cb", "Out-of-Range by Dataset")
p8 <- plot_invalid(df_type, "pct_nonint", "#e78ac3", "Non-Integer by Dataset")

p9  <- plot_invalid(df_group, "pct_na", "#66c2a5", "NA by Group Size")
p10 <- plot_invalid(df_group, "pct_text", "#fc8d62", "Text Errors by Group Size")
p11 <- plot_invalid(df_group, "pct_range", "#8da0cb", "Out-of-Range by Group Size")
p12 <- plot_invalid(df_group, "pct_nonint", "#e78ac3", "Non-Integer by Group Size")

# 7. Arrange with patchwork

```


We then compute the **percentage of each invalid type** within the following groupings:

- `model_short`: the language model used
- `dataset`: the moral dilemma source
- `group_size`: the number of agents (e.g., solo, pair, triad)

The helper function `get_invalid_summary()` calculates proportions of each invalid category by group.  
Next, `plot_invalid()` generates labeled bar plots for each error type, and the `patchwork` package combines them into visual grids.

#### Visualization Grids

- **Grid 1**:
  - Shows `is_text`, `out_of_range`, and `non_integer` values by `model_short`
  - Also shows the same metrics by `group_size`



```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.height=5, fig.width=8}
p <- ((p2 | p3 | p4) /
 ( p10 | p11 | p12)) +
  plot_layout(heights = c(2, 2, 2))
print(p)
```


- **Grid 2**:
  - Focuses on `is_na`, `is_text`, and `out_of_range` errors grouped by `dataset`

These visualizations help us assess data quality and detect group-specific error patterns, guiding further data cleaning and informing our downstream analysis pipeline.


```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.height=5, fig.width=8}
# 7. Arrange with patchwork
 
p <- ( p6 / p7 / p8) 
  plot_layout(heights = c(1, 1, 1))
print(p)
```



# Step 4: Understanding Non-Integer Entries and Rounding Them

This step investigates how many of the `opinion` values are non-integer, both before and after cleaning, and visualizes where they occur.

We first extract the `opinion` column from `combined_df` and create its numeric version (`opinion_num`). We then define several invalid conditions:
- `is_na`: missing entries
- `is_text`: non-numeric values
- `out_of_range`: values outside 1–7
- `non_integer`: values like 3.5 that are valid numbers but not whole

These conditions help us assess the reliability of user responses.

We then summarize the percentage of each invalid type by:
- `model_short` (e.g., GPT4.1, Qwen3)
- `dataset` (e.g., oxford, greene)
- `group_size` (e.g., solo, pair)

The plots show the frequency of non-integer values in each group. This helps us understand whether rounding was necessary and whether certain models or conditions produce more inconsistent inputs.

```{r}
# 1. Extract relevant column
opinion_char <- as.character(combined_df$opinion)
opinion_num  <- suppressWarnings(as.numeric(opinion_char))

# 2. Define invalid types
is_na        <- is.na(combined_df$opinion)
is_text      <- !str_detect(opinion_char, "^-?\\d+(\\.\\d+)?$")
out_of_range <- !is.na(opinion_num) & (opinion_num < 1 | opinion_num > 7)
non_integer  <- opinion_num %% 1 != 0 & !is.na(opinion_num)
invalid_all  <- is_na | is_text | out_of_range

# 3. Total summary
total_rows    <- nrow(combined_df)
missing_count <- sum(is_na)
text_count    <- sum(is_text)
range_count   <- sum(out_of_range)
nonint_count  <- sum(non_integer)
invalid_total <- sum(invalid_all)

# 4. Overall stats
cat(sprintf("Total rows: %d\nNA: %d | Text: %d | Out of range: %d | Non-integer: %d | Total Invalid: %d (%.1f%%)\n\n",
            total_rows, missing_count, text_count, range_count, nonint_count,
            invalid_total, 100 * invalid_total / total_rows))

# 5. Summarise by model_short, dataset, group_size
by_model <- combined_df %>%
  group_by(model_short) %>%
  summarise(
    pct_na = 100 * sum(is.na(opinion)) / n(),
    pct_text = 100 * sum(is_text) / n(),
    pct_range = 100 * sum(out_of_range) / n(),
    pct_nonint = 100 * sum(non_integer) / n(),
    .groups = "drop"
  )

by_dataset <- combined_df %>%
  group_by(dataset) %>%
  summarise(
    pct_na = 100 * sum(is.na(opinion)) / n(),
    pct_text = 100 * sum(is_text) / n(),
    pct_range = 100 * sum(out_of_range) / n(),
    pct_nonint = 100 * sum(non_integer) / n(),
    .groups = "drop"
  )

by_group_size <- combined_df %>%
  group_by(group_size) %>%
  summarise(
    pct_na = 100 * sum(is.na(opinion)) / n(),
    pct_text = 100 * sum(is_text) / n(),
    pct_range = 100 * sum(out_of_range) / n(),
    pct_nonint = 100 * sum(non_integer) / n(),
    .groups = "drop"
  )

p_nonint_model <- ggplot(by_model, aes(x = reorder(model_short, pct_nonint), y = pct_nonint)) +
  geom_col(fill = "#fc8d62") +
  geom_text(aes(label = sprintf("%.1f%%", pct_nonint)), vjust = -0.5, size = 3) +
  labs(x = "Model", y = "Non-integer %", title = "Non-integer Values by Model") +
  scale_y_continuous(expand = expansion(c(0, 0.1)), labels = percent_format(scale = 1)) +
  theme_minimal()

p_nonint_dataset <- ggplot(by_dataset, aes(x = reorder(dataset, pct_nonint), y = pct_nonint)) +
  geom_col(fill = "#a6d854") +
  geom_text(aes(label = sprintf("%.1f%%", pct_nonint)), vjust = -0.5, size = 3) +
  labs(x = "Dataset", y = "Non-integer %", title = "Non-integer Values by Dataset") +
  scale_y_continuous(expand = expansion(c(0, 0.1)), labels = percent_format(scale = 1)) +
  theme_minimal()

p_nonint_group <- ggplot(by_group_size, aes(x = reorder(group_size, pct_nonint), y = pct_nonint)) +
  geom_col(fill = "#e78ac3") +
  geom_text(aes(label = sprintf("%.1f%%", pct_nonint)), vjust = -0.5, size = 3) +
  labs(x = "Group Size", y = "Non-integer %", title = "Non-integer Values by Group Size") +
  scale_y_continuous(expand = expansion(c(0, 0.1)), labels = percent_format(scale = 1)) +
  theme_minimal()
```

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.height=5, fig.width=8}
p <- (p_nonint_model | p_nonint_dataset) / p_nonint_group + 
  plot_layout(heights = c(3, 2))
print(p)
```


Next, we validate the final cleaned dataset `combined_df_clean`. We again check for:
- Empty strings or named NA values (e.g., `"None"`, `"missing"`)
- Non-numeric or out-of-range entries
- Non-integer values

Finally, we verify the integrity of the cleaned dataset:
- `combined_dataset` stores the finalized version
- We summarize its structure using `summary()` and `which(is.na())` to double-check that no unexpected NA values remain

This ensures `combined_dataset` is clean, complete, and safely usable for statistical modeling and group comparisons.


Next, we validate the final cleaned dataset `combined_df_clean`. We again check for:
- Empty strings or named NA values (e.g., `"None"`, `"missing"`)
- Non-numeric or out-of-range entries
- Non-integer values

Finally, we verify the integrity of the cleaned dataset:
- `combined_dataset` stores the finalized version
- We summarize its structure using `summary()` and `which(is.na())` to double-check that no unexpected NA values remain

This ensures `combined_dataset` is clean, complete, and safely usable for statistical modeling and group comparisons.

```{r echo=FALSE}
library(dplyr)
library(stringr)

# 1. Coerce to character and numeric
opinion_char <- as.character(combined_df_clean$opinion)
opinion_trim <- str_trim(opinion_char)
opinion_num  <- suppressWarnings(as.numeric(opinion_trim))

# 2. Define invalid conditions
is_na            <- is.na(combined_df_clean$opinion)
is_blank         <- opinion_trim == ""
is_named_na      <- tolower(opinion_trim) %in% c("na", "none", "null", "missing")
is_non_numeric   <- is.na(opinion_num) & opinion_trim != "" & !is_named_na
is_out_of_range  <- !is.na(opinion_num) & (opinion_num < 1 | opinion_num > 7)
is_not_integer   <- !is.na(opinion_num) & (opinion_num %% 1 != 0)

# 3. Combine all invalid flags
invalid_all <- is_na | is_blank | is_named_na | is_non_numeric | is_out_of_range | is_not_integer

# 4. Print summary
cat("Total rows:                ", nrow(combined_df_clean), "\n")
cat("NA values (real NA):       ", sum(is_na), "\n")
cat("Blank strings:             ", sum(is_blank), "\n")
cat("Named NAs (e.g., 'None'):  ", sum(is_named_na), "\n")
cat("Non-numeric strings:       ", sum(is_non_numeric), "\n")
cat("Out of 1–7 range:          ", sum(is_out_of_range), "\n")
cat("Not integer (e.g., 3.5):   ", sum(is_not_integer), "\n")
cat("Total invalid entries:     ", sum(invalid_all), "\n\n")


# 5. Show rows with any invalid entries
invalid_rows <- combined_df[invalid_all, ]
print(invalid_rows)
```



# Step 5: Splitting Datasets by Source

To enable dataset-specific analyses, we split the cleaned dataset `combined_dataset` into separate subsets based on the `dataset` column.

The resulting data frames are:
- `greene_df`: items from Greene et al.’s classic moral dilemma study
- `keshmirian_df`: items from Keshmirian et al.’s dataset
- `korner_df`: items developed by Körner et al.
- `oxford_df`: items from the Oxford Utilitarianism Scale
- `cni_df`: items using the CNI (Consequences, Norms, Inaction) moral framework

Each subset is filtered using `filter(dataset == "...")` and passed through `droplevels()` to remove any unused factor levels.

This step prepares each dataset for detailed, dataset-specific analysis, helping ensure that summaries, models, and visualizations reflect only the relevant subset of data.
"""


```{r}
combined_dataset=combined_df_clean

greene_df  <- combined_dataset %>% filter(dataset == "greene")%>%
  droplevels()
keshmirian_df  <- combined_dataset %>% filter(dataset == "keshmirian")%>%
  droplevels()
korner_df      <- combined_dataset %>% filter(dataset == "korner")%>%
  droplevels()
oxford_df      <- combined_dataset %>% filter(dataset=="oxford")%>%
droplevels()

cni_df  <- combined_dataset %>% filter(dataset=="cni")%>%
droplevels()

```


```{r echo=FALSE}
library(dplyr)
library(ggplot2)
library(scales)
library(patchwork)

# A helper that takes one dataset and returns the 4 plots
inspect_balance <- function(df, dataset_name) {
  # drop any unused factor levels
  df <- df %>% droplevels()
  
  # restrict to non‐missing opinions
  df0 <- df %>% filter(!is.na(opinion))
  
  # 1) By type
  sum_type <- df0 %>%
    count(type) %>%
    mutate(pct = n/sum(n)*100)
  p_type <- ggplot(sum_type, aes(type, n)) +
    geom_col(fill="#66c2a5") +
    geom_text(aes(label=sprintf("%d\n(%.1f%%)", n, pct)), 
              vjust=-0.5, size=3) +
    labs(title=paste(dataset_name, "– by Type"),
         x="Type", y="Count") +
    theme_minimal() +
    theme(axis.text.x=element_text(angle=45, hjust=1))
  

  
  # 3) By model
  sum_model <- df0 %>%
    count(model) %>%
    mutate(pct = n/sum(n)*100)
  p_model <- ggplot(sum_model, aes(model, n)) +
    geom_col(fill="#8da0cb") +
    geom_text(aes(label=sprintf("%d\n(%.1f%%)", n, pct)),
              vjust=-0.5, size=3) +
    labs(title=paste(dataset_name, "– by Model"),
         x="Model", y="Count") +
    theme_minimal() +
    theme(axis.text.x=element_text(angle=45, hjust=1))
  
  # 4) By model × type
  sum_mt <- df0 %>%
    count(model, type) %>%
    group_by(model) %>%
    mutate(pct = n/sum(n)*100) %>%
    ungroup()
  p_mt <- ggplot(sum_mt, aes(type, n, fill=model)) +
    geom_col(position="dodge") +
    geom_text(aes(label=sprintf("%d\n(%.1f%%)", n, pct)),
              position=position_dodge(width=0.9),
              vjust=-0.5, size=2.5) +
    labs(title=paste(dataset_name, "– by Model × Type"),
         x="Type", y="Count", fill="Model") +
    theme_minimal() +
    theme(axis.text.x=element_text(angle=45, hjust=1))
  
  # assemble
  (p_type | p_model) /
  ( p_mt) +
    plot_annotation(title = paste("Balance check:", dataset_name))
}

# 1) split off
greene_df      <- combined_dataset %>% filter(dataset=="greene")
keshmirian_df  <- combined_dataset %>% filter(dataset=="keshmirian")
korner_df      <- combined_dataset %>% filter(dataset=="korner")

# 2) run for each
inspect_balance(greene_df,     "Greene")
inspect_balance(keshmirian_df, "Keshmirian")
inspect_balance(korner_df,     "Korner")
inspect_balance(oxford_df,     "oxford")
inspect_balance(oxford_df,     "cni")
```

```{r save-rds, echo=FALSE}
# Save
saveRDS(combined_dataset, file = "combined_dataset.rds")
```


















